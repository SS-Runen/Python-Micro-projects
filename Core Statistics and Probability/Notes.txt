++++++++++++++++++
General Procedures

-- Procedure When There are Too Many Missing Values
- Run a simulation that includes a sample size value
- When operating on percentages without any unit change (e.g. percentage to decimal), keep the unit symbol, as in 20% * 0.05 when getting 5% of 20% and expressing the final answer in percentage (1%).

-- Interpreting Questions and Fitting to Formulas
- Check for "domain knowledge" or other circumstances that could change the values or formulas you need.
- Run a simulation of worded problems and match the formula and values with steps or actions where possible.
- Review your work.

++++
Data

Continuous data: Data for which the granularity of the measurement can be changed. These values could have an infinite range of granularity.
Categorical data: Values are set and there are no "in-between" data values/ranges.

Levels of Data Measurement:
Nominal - Categorical and cannot be ordered/sorted.
Ordinal - Categorical and can be ordered/sorted. Cannot provide scale, i.e. 1 vs. 2 vs. 6.
Interval - Provides scale and lacks a zero point/floor. Degrees Celsius/Fahrenheit have no "true zero point" or floor. Kelvins have a true zero point since Kelvin does not measure temperature with negative values.
Ratio - Provide scale, can be ordered (just like interval levels of measure) and have a true zero point. E.g. age, population.

Population - All members of a certain group.
Sample - A subset of a specific group.

Covariance - The "direction" of the relationship between two variables, i.e. a value that indicates whether two variables are positively or negatively/inversely correlated. It cannot accurately tell the strength of the correlation. The more distant (negatively or positively) from zero, the higher the covariance value. To use covariance in measuring the degree/strength of correlation, the values have to be normalized/scaled based on each sets values relative magnitude. Refer to:
https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/
https://www.investopedia.com/terms/c/covariance.asp#:~:text=Covariance%20measures%20the%20direction%20of%20the%20relationship%20between%20two%20variables,other%20tends%20to%20be%20low.

Pearson Correlation Coefficient - Measures the strength of the association or relation of two variables. It gives the magnitude and direction of a correlation. Refer to:
https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/pearsons-correlation-coefficient/

++++++++++++++++++++++++
Properties of Factorials

Negative numbers do not have a factorial.

The value of 0! is set at 1.

n! = (n-1)! * n

(n + 1)! = n! * n

(n + k)! = n! * (n + 1) * ...(n + k)

(n - k)! = n! / ((n - k + 1) * (n - k + 2) * ...(n - k + k)) or n! / ((n - k + 1) * (n - k + 2) * ...n)

Assuming n > k, n! / k! = (k + 1) * (k + 2) * ...n

+++++++++++++
Combinatorics

Variation with Repetition/Replacement - Assumes a set with n number of elements permuted p at a time but each element is replaced/may be used repeatedly. In other words, filling a number of "slots" or "positions" with elements from a certain set while allowing repeated use of any element. v = n^p where n is the number of available elements and p is the number of positions to fill.

Variations without Repetition - Denoted by v = n! / (n-p)! where n is the number of usable elements and p is the number of positions to fill. Synonymous to "permutations of n number of elements taken p at a time without replacement." Note that n! denotes the total permutations of all usable elements. (n - p)! denotes permutations of elements that are not going to be used to fill in positions. This implies that a variation without replacement is the number of times the permutation of unused elements can be subtracted from the permutation of all elements.

Combinations - To get the number of unique sets/combinations (C) for a number of elements (n) taken (p) at a time, nCp = n! / (p! * (n-p)!). Similar to permutations, the (n-p)! in the denominator removes the permutations of elements that will not be used. The additional p! * ... removes the permutations of the different elements from the result set.

Symmetry of Combinations - n C r = n C (n - r). E.g. 10C7 = 10C3.

++++++++++++++++++++++++++++
Measures of Central Tendency

Mean/Arithmetic Mean - The sum of values in a set divided by the count of values. It can be influenced significantly by outliers. Population mean is usually denoted by μ and sample mean by x or x-bar.

Geometric mean - The product of all values in a set raised to the (1/n)th power, with n being the number of values in the set. μ = (n1 * n2 * ...n)^(1/n) or μ = n√(n1 * n2 * ...n). The term 1/n denotes the nth root, e.g. 1/2 denotes the square root. This mean is often used to calculate growth over time. It can be referred to as the time-weighted rate of return. Unlike the arithmetic mean, the geometric mean can be used to measure compounding. E.g. for [2, 18], geometric μ = (2*18)^1/2. For [2, 18, 20, 25], geometric μ = (2*18*20*25)^(1/4).

Median - The middle value or the average of the two middle values of an ordered set.

Mode - The most frequent value in a set.

The mean is usually more affected by outliers with a high absolute value.

Percentile - A value which a certain percentage of values in a set are less than. I.e. the 10th percentile denotes a value compared to which 10% of all values in the set are lower.

Quartile - Usually arbitrarily set at the 25th, 50th, and 75th percentiles.

++++++++++++++++++++++
Measures of Dispersion

Range - Difference between maximum and minimum value of a set.

Variance - The average of the squared distances from the mean of all data points. The sum of squared distance from the mean of each point divided by either sample size less one (n - 1) or population size. Sample variance s^2 requires the use of (n - 1) as denominator. Population variance σ^2 uses population count n. Here, n is the number of data points/elements in the sample or population.

Standard Deviation - The square root of a sets variance. The set may be a sample s or population σ.

*Bessel's Correction:
- The use of (n - 1) as a denominator to reduce bias due to finite sample size.

Inter-quartile Range - A basis for calculating a fence for marking outliers. The fence is often arbitrarily set to 1.5 times the inter-quartile range (IQR). IQR is the distance between the lower limit in quartile 1 and the upper limit in quartile 3. Outliers are either higher by 1.5 times the IQR compared to the lower limit of the 3rd quartile or lower by 1.5 times the IQR compared to the lower limit of the 1st quartile.

Z-score - The number of standard deviations from the mean to a certain value.

Coefficient of Variation - This is an indicator of how much "spread" your data has. Standard deviation divided by mean and multiplied by 100%. C = (standard_deviation / mean) * 100%.

+++++++++++
Probability

Union - The probability of either A or B occuring. Calculated using the formula for the Addition Rule for Probabilities.

Intersection - The probability that two events/results A and B both occur, i.e. both hypotheses are simultaneously true. For independent events, multiply prbability of A with that of B to get the probabilities. P(A ∩ B) = P(A) * P(B).

Addition/Additive Rule for Probabilities - The probability of A or B occuring, P(A U B), is given by: P(A U B) = P(A) + P(B) - P(A ∩ B). For mutually exclusive events, ommit `P(A ∩ B)`. P(A U B) = P(A) + P(B).

Multiplication Rule for Independent Events - If A and B are independent, P (A ∩ B) = P(A) * P(B)

Multiplication Rule for Dependent Events - Where event A is dependent on B, i.e. B -> A, P(A|B) = P(A) * P(B|A). This may be rephrased as the probability of A given that B has occured. The formula is equivalent to: P(A|B) = P(A ∩ B) / P(B). From that formula, P(A ∩ B) = P(A|B) * P(B).

Law of Total Probability - If A is the union of a finite number of events (B1 U B2 U Bn...), P(A) = (P(A|B1) * P(B)) + (P(A|B2) * P(B2)) +...

Expected Outcome = The expected numerical result E of an experiment for an event A. Denoted as E(A). For categorical results, E(A) = P(A) * n, where n is the number of trials performed.

- Conditional Probability

-- For dependent events A and B

The probability of B given A, i.e. P(B) if A has happened:
P(B|A) = P(B ∩ A) / P(A)
~ The probability of B given A is equal to the probability of B intersection A divided by the probability of A.

The probability of A given B, i.e. P(B) happening if A has happened/will happen:
P(A|B) = P(B ∩ A) / P(B)
~ The probability of A given B is equal to the probability of A intersection B divided by the probability of B.

* The probability of the event "taken for granted" or assumed to happen will be the denominator of the probability of A intersection B.

-- For two events A and B, if the probability of A is the same as the probability of A given B, the events are independent. This is because dependent events shouldn't affect each others probability.
P(A) = P(A|B) ~ independent
P(B) = P(B|A) ~ independent
P(A) != P(A|B) ~ dependent

-- Because independent events do not affect each others probability, the intersection of independent events A and B should equal the probability of A multiplied to the probability of B.
P (A ∩ B) = P(A) * P(B) ~ independent

- Baye's Theorem
P(B|A) = P(A|B)*P(B) / P(A)

- Functions of Random Variables
Given that the function expected value (μ) = b * X + a where a is a constant, and X is a random variable, μ is a linear equation. The equation standard deviation can be calculated by multiplying the absolute value of the equation's slope and the SD of X:
SD = |b| * SD(X)

++++++++++++++++++++++
Discrete Distributions

Binomial Distribution Probability Mass Function - Describes the probability of observing x successes in total after performing n trials. The term (n x) denotes a combination of n taken x at a time. Note that n! / (x! * (n-x)!) is also the formula for calculating combinations. Specifically, it is the formula for n elements combined x at a time: nCx.

P(x) = (n! / (x! * (n-x)!)) * P^x * (1-P)^(n-x)
Here,
P(x): probability of x occuring given that it is a binomial distribution.
P: probability of x occuring during one trial, i.e the probability of the preferred outcome/success.
n: number of trials.
x: desired count of outcomes.

This formula can be accessed in Excel or Google sheets using =BINOM.DIST.RANGE().

For binomial probability distributions, P(x) should be consistent for each trial or member event. The trials should be independent of each other.

Variance σ2 = n * P * (1-P)
Expected Value for Independent Events n to z: (n * P(n)) + (m * P(m))...(z * P(z))
Expected Value of a Binomial Probability Distribution: P(x) * n

Binomial Probability Distribution Table - This table lists the intersections between the number of trials, P, and x. It is a cumulative probability distribution. To get the probability for a specific x, take its cumulative probability and subtract the undesired outcomes. I.e, subtract the cumulative probability of the opposite "undesirable" condition. E.g.: if you need the probability of getting x >= 5, subtract the probability of getting x <= 4 from 1. If you need the probability of a specific x, say x = 5, subtract the probability of x <= 4 from the probability of x <= 5.

Poisson Disribution Probability Mass and Cumulative Mass Function - Probability of getting x successes per some continuous unit, e.g. time or distance. To get the probability of x or less/more successes, simply sum the poisson distributions of x and below or x and above. The poisson distribution assumes uniformity of the intervals between success. If there is a reason for events or successes to occur with varying intervals - e.g. at a the end of 5 PM and during the first minutes of 8 AM - the poisson distribution cannot be used effectively.

++++++++++++++++++++++++
Continuous Distributions

- Normal/Guassian/Bell-curve distribution - Maps probabilities to an area x. Probabilities cannot be mapped to a single specific outcome and have to be mapped to a range or interval instead. In a continuous normal distribution, the mean, median and mode are equal. Regardless of what the mean, median, or mode are, all normal distributions are centered and symmetric around the mean.

Standard Normal Distribution - A normal distribution with a mean of 0 and standard deviation of 1.

The area under the curve of a normal probability distribution is always equal to one. This applies to distributions where probabilities are used as the values.
Around 68% of values within one standard deviation - i.e. one multiplied by the value of the standard dviation - less and more than the mean.
About 95% of values fall within two standard deviations from the mean in either direction/magnitude.

-- Z-score
A z-score is the number of standard deviations from the mean for values in a normal distribution. If a data point has a z-score of 0.3, it is (0.3 * s) or (0.3 * σ) away from the mean. A z-score can be used to approximate your percentile.
Z-scores have "direction". They can be negative.
Z = (x - μ) / σ
or
Z = (x - μ) / s

Z-table - This table shows the percentage of values in a normal distribution that fall to the left of a score of the specified Z-value. I.e., a z-table shows the percentage of scores in a normal distribution that are lower than the value at a specified z-score. In other words, a z-table shows the percentile of a score based on the z-value.

++++++++++++++++++++++
Joint Random Variables

Confidence Interval - A confidence interval is a range of values with a certain probability to include an unknown parameter. E.g. a confidence interval of 10 - 20 meters with a confidence level of 90% means that a result from a certain population or sample is 90% likely to fall within 10 to 20 meters.

Alpha value - The alpha value is the probability of rejecting the null hypothesis when the null hypothesis is true. In relation to a confidence interval, the alpha value can be defined as the probability that an unknown value will not be included in the confidence interval. There are equations for setting the alpha value and some arbitrary guidelines. It may be expressed as 1 - Confidence. If the confidence for a certain interval is 50%, the probability of a value outside the interval is also 50%. The interval is usually set as the alternate hypothesis. The alpha indicates the probability of rejecting the null hypothesis despite the null hypothesis being true.

The universe probabilities for two sets of random variables X and Y refers to all combinations of values for X and Y.

Correlation - Correlation measures the relation between two variables. A correlation of positive or negative one indicates a perfect positive or negative linear correlation between two variables. E.g. the radius of a circle and the circumference of a circle, temperature in Fahrenheit and temperature in Celsius, amount of charitable donations and tax exemption amount. A correlation of zero means two variables are independent.

Covariance Equation:

Cov(X,Y) = Σ E((X – μ) E(Y – ν)) / n-1
Here:
X - random variable
E(X) is the expected value (the mean) of the random variable X and
E(Y) is the expected value (the mean) of the random variable Y
n - number of items in the data set.

Alternative Covariance Equation:
Cov(X, Y) = E[XY] - μx * μy
Here:
E[XY] - Expected value/average of X * Y
μx - Mean of X
μy - Mean of Y

Correlation Equation:
Corr(X, Y) = Cov(X, Y) /  (σx σy)
Here:
σx or sx - standard deviation (SD) of X.
σy or sy - SD of Y.

Related Equations:

Expected Value of X + Y:
E[X + Y] = μx + μy

Expected Value of X - Y:
E[X - Y] = μx - μy

Weighted Expected Values of the sum/difference of X and Y:
E[aX + bY] = a*μx + b*μy
E[aX - bY] = a*μx + b*μy

Variance of X + Y:
Var(X + Y) = σx^2 + σy^2 + 2Cov(X, Y)

Variance of X - Y:
σ^2 = σx^2 + σy^2 - 2Cov(X, Y)

In the last two equations:
σx - SD of x
σy - SD of y

Weighted Variance of the sum of X and Y:
σ^2(aX + bY) = (a^2 * σx^2) + (b^2 * σy^2) + 2*a*b*Cov(X, Y)
or
Variance(aX + bY) = (a^2 * σx^2) + (b^2 * σy^2) + 2*a*b*Corr(X, Y)*σx*σy
Here:
a, b - weights.
σx - standard deviation (SD) of x.
σy - SD of y.


++++++++
Sampling

- An arbitrary minimum sample count is > 30.

Selection Bias - Bias toward selecting samples more likely to react or respond to polls, surverys, and other data gathering requests or sampling techniques. Examples: Undercoverage Bias: omitting a segment of the population. Self-selection bias: A bias caused by differences between people who volunteer themselves as samples compared to those who don't volutneer.

Survivorship Bias - This happends when samples that are inferior, damaged, or unobservable are removed or are not included in the sample pool.

Random Sampling - All population members have an equal chance of being selected.
Staratifed Sampling - A population is sorted into segments based on certain characteristics. Members cannot belong to two or more groups. From each group, take a sample. The sample size from each group must have the same ratio to the total sample size as the group's ratio to the total population.

Clustering - The population is broken into groups and samples are taken from a random selection of groups. The group sample size ratio to total sample size must be the same as group size ratio to population size.

+++++++++++++
Miscellaneuos

@@@@@
Setup

- Install latest version of Python. Ubuntu version usually lags behind latest stable version. Download, unzip, and process latest tar file.
Source:
https://phoenixnap.com/kb/how-to-install-python-3-ubuntu

- Create a virtual environment. Do not do anything past creating an environment.
https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-20-04


- Install helper modules, especially pip.

- To use an environment, navigate to the environments folder and enter this command:
source ./bin/activate


