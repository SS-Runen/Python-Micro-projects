++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using some input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

+++++++++++++++++
Linear Regression

For a cost function J with parameters w and b, J(w, b), continuously update w and b to minimize J.
w = w - α (∂/∂w)J(w,b)
and
b = b - α (∂/∂b)J(w,b)
Where:
α = Learning rate. This is usually a decimal value between 0 and 1. It functions as a multiplier to the amount of change.
(d/dw)J(w,b) = This entire term represents the value of the partial derivative of the function J with parameters w and b. The term "d" indicates to change the value of w when looking for the derivative and to keep other values constant.

* A derivative is the rate of change of a function relative to a variable. A derivative may be noted as f' or as d/dx. A partial derivative may be noted as ∂/∂x or f. The term x can be any variable. Calculating a derivative involves finding the slope of a point in a line using the properties of tangents, limits, and triangle trigonometry.
* The update operations of w and b have to be done at the same time and in step. When performing the descent, the current/old values should be used as input to J. The updated values will only be used in the next iteration.

+++++++++++++++++++
Logistic Regression

Sigmoid Function
- a.k.a Logistic Function. It outputs values between zero and one given z.
g(z) = 1 / (1 + e^-z)

- The output of a linear or polynomial regression model can be used as the input (z) to a logistic/sigmoid function. The result of a logistic function is the probability of a positive result, i.e. the sigmoid function gives the probability that the target characteristic is present for the sample/vector x.
f(w, b) = P(y = 1 | x; w, b)

Decision Boundary
- The decision boundary is the value at which the probability between g(z) = 1 or g(z) = 0 is equal or neutral. g(w,x) = w1x1 +...wnxm = 0. The decision boundary may be calculated by substituting values into the input equation and then solving for or simplifying them. 

Logarithm
- A logarithm is the exponent or power to which a base must be raised to yield a given number. E.g. the logarithm of 8 with base 2 is 3. Two must be raised to the exponent three to yield eight.

Simplified Cost Function for Logistic Regression
- Refer to Google Drive notes. This function is convex (only has one global minimum) and uses the statistical principle of maximum likelihood.

+++++++++++++++++++++++++++++++++++++++
Gradient Descent with Linear Regression

These are the expanded formulas for a cost function used in single-variable linear regression.
w = w - α * (1/m) * Σ(f<-w,b(wx^i + b) - y^i) x^i
b = b - α * (1/m) * Σ(f<-w,b(wx^i + b) - y^i)

* Due to the rules of calculus, the x^i term happens to be removed when calculating the update to b.

- A property of gradient descent cost functions for linear regression is that they do not have multiple local minima. This kind of function is called a convex function, i.e. a function with only one global minimum.

Convergence Checking
- Check if the error or cost function decreases by a certain amount, ε. Compare the error optimization of different learning rates (α) over a small amount of runs (e.g. 30) and gradually change α.

Feature Engineering for Regression
- Features may be removed automatically by the model. Another way is to use domain knowledge to transform or combine data into new features and compare their performance to a certain baseline or "control" set of features. You may also compare the effect of new or aggregated features to the separate source features.

Polynomial Regression
- You may derive a new feature from x by taking its various powers - usually the second or third - or taking x' square root. This may not be applicable to some features with negative values. For negative values, it may be useful to transform the absolute value and then multiply it to a negative value as part of the transformation process.

+++++++++++++++
Feature Scaling

Z-score normalization - Use the z-scores as x-values.
x = (x - μ) / σ

Min-max normalization
x = x / (max(vectorX))

Mean normalization
x = (x - μ) / (max(vectorX) - min(vectorX))

- If certain features have very large or very small x-values, the gradient descent model may take longer to reach a low-value cost function. The user has low control over the amount of change to a parameter in the early stages of the cost function optimization. Values with an uneven scale may force the model to "bounce" around large ranges and take longer to minimize error.

- When normalizing, it is recommended to have values within a small countable negative and positive range to retain magnitude and direction versus some reference point.

++++++++++++++++++++++++++++
Overfitting and Underfitting

- If a model fails to capture or reflect the relationships between parameters and input strongly enough, its predictions will not change properly with respect to new or real-world input. This is underfit model is said to have high bias, similar to having a "preconception" and ignoring data that goes against the preconception.

- When a model is overfitted, it responds too strongly and too specifically to training data. Due to the mathematics of the model, any slight deviation of the input from the training data will cause very inaccurate predictions. Slightly different data sets can give very different results, so an overfit model is said to have high variance.

- A good model adjusts predictions based on input without giving erroneous results when presented real-world data or proper data from the same population. This model is a "generalized" model.

- Overfit may be caused by excess parameters that hint at relationships which don't exist in the population. Excess parameters may also cause underfit due to noise. The model could miss the proper cost function minimization parameters since it does not have good feedback.

