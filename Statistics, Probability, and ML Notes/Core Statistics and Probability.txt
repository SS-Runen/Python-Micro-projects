++++++++++++++++++
General Procedures

- - Procedure When There are Too Many Missing Values
- Run a simulation that includes a sample size value
- When operating on percentages without any unit change (e.g. percentage to decimal), keep the unit symbol, as in 20% * 0.05 when getting 5% of 20% and expressing the final answer in percentage (1%).
- When possible, use 0 (zero) as the replacement or stand-in value for variables you assume to be the same or insignificant. When certain values are not given but are said to be equal, zero should be their default value.

- - Interpreting Questions and Fitting to Formulas
- Check for "domain knowledge" or other circumstances that could change the values or formulas you need.
- Verify if the number of possible outcomes is too high or the problem is too complex for a case-by-case evaluation. If it is, use formulas instead.
- Run a simulation of worded problems and match the formula and values with steps or actions where possible.
- Review your work.

- Before a break, check the new notes made using GitHub Desktop while reviewing the video or activity.
- Symbols may be searched for and copied from Wumbo.net:
https://wumbo.net/symbols/
- To see the function prameters of a Google Sheet formula, click the question mark tooltip. It is usually on the left side of the cell.

++++
Data

Continuous data: Data for which the granularity of the measurement can be changed. These values could have an infinite range of granularity.
Categorical data: Values are set and there are no "in-between" data values/ranges.

Levels of Data Measurement:
Nominal - Categorical and cannot be ordered/sorted.
Ordinal - Categorical and can be ordered/sorted. Cannot provide scale, i.e. 1 vs. 2 vs. 6.
Interval - Provides scale and lacks a zero point/floor. Degrees Celsius/Fahrenheit have no "true zero point" or floor. Kelvins have a true zero point since Kelvin does not measure temperature with negative values.
Ratio - Provide scale, can be ordered (just like interval levels of measure) and have a true zero point. E.g. age, population.

Population - All members of a certain group.
Sample - A subset of a specific group.

Covariance - The "direction" of the relationship between two variables, i.e. a value that indicates whether two variables are positively or negatively/inversely correlated. It cannot prove causation on its own. The more distant (negatively or positively) from zero, the higher the covariance value. To use covariance in measuring the degree/strength of correlation, the values have to be normalized/scaled based on each sets values relative magnitude. Covariance requires that input values are NORMALIZED.
Refer to:
https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/
https://www.investopedia.com/terms/c/covariance.asp#:~:text=Covariance%20measures%20the%20direction%20of%20the%20relationship%20between%20two%20variables,other%20tends%20to%20be%20low.

Basic Covariance Equation:
Cov(x, y) = (1.N) * Σ (xi - x̄)(yi - ȳ)

Pearson Correlation Coefficient - Measures the strength of the association or relation of two variables. It gives the magnitude and direction of a correlation. Refer to:
https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/pearsons-correlation-coefficient/

ρ(x, y) = Cov(x, y) / (σy * σy)

++++++++++++++++++++++++
Properties of Factorials

Negative numbers do not have a factorial.

The value of 0! is set at 1.

n! = (n-1)! * n

(n + 1)! = n! * n

(n + k)! = n! * (n + 1) * ...(n + k)

(n - k)! = n! / ((n - k + 1) * (n - k + 2) * ...(n - k + k)) or n! / ((n - k + 1) * (n - k + 2) * ...n)

Assuming n > k, n! / k! = (k + 1) * (k + 2) * ...n

+++++++++++++
Combinatorics

Variation with Repetition/Replacement - Assumes a set with n number of elements permuted p at a time but each element is replaced/may be used repeatedly. In other words, filling a number of "slots" or "positions" with elements from a certain set while allowing repeated use of any element. v = n^p where n is the number of available elements and p is the number of positions to fill.

Permutations - With no subset and no repetition allowed, P = n!. For sets with n members permuted r at a time with no repetition allowed,
nPr = n! / (n-r)!.
Count the times that (n-r)! can be removed from n!. If repetition is allowed, nPr = n^r.

Variations without Repetition - Denoted by v = n! / (n-r)! where n is the number of usable elements and r is the number of positions to fill. Synonymous to "permutations of n number of elements taken p at a time without replacement." The n! denotes the total permutations of all usable elements. (n - r)! denotes permutations of elements that are not going to be used to fill in positions. This implies that a variation without replacement is the number of times the permutation of unused elements can be subtracted from the permutation of all elements.

Combinations - In a combination, order is not significant. To get the number of unique sets/combinations (C) for a number of elements (n) taken (p) at a time without repetition,
nCr = n! / (r! * (n-r)!).
Similar to permutations, the (n-r)! in the denominator removes the permutations of elements that will not be used. The additional r! *... removes the permutations of the different elements from the result set.
For combinations that allow repetition of elements,
n+r-1Cr = (n + r-1)! / (r! * (n-1))

Symmetry of Combinations - n C r = n C (n - r). E.g. 10C7 = 10C3.

++++++++++++++++++++++++++++
Measures of Central Tendency

Mean/Arithmetic Mean - The sum of values in a set divided by the count of values. It can be influenced significantly by outliers. Population mean is usually denoted by μ and sample mean by x or x-bar.

Geometric mean - The product of all values in a set raised to the (1/n)th power, with n being the number of values in the set. μ = (n1 * n2 * ...n)^(1/n) or μ = n√(n1 * n2 * ...n). The term 1/n denotes the nth root, e.g. 1/2 denotes the square root. This mean is often used to calculate growth over time. It can be referred to as the time-weighted rate of return. Unlike the arithmetic mean, the geometric mean can be used to measure compounding. E.g. for [2, 18], geometric μ = (2*18)^1/2. For [2, 18, 20, 25], geometric μ = (2*18*20*25)^(1/4).

Median - The middle value or the average of the two middle values of an ordered set. A large difference between the mean and median could be a sign that there are outliers with unusually strong effects on the mean.

Mode - The most frequent value in a set.

The mean is usually more affected by outliers with a high absolute value.

Percentile - A value which a certain percentage of values in a set are less than. I.e. the 10th percentile denotes a value compared to which 10% of all values in the set are lower.

Quartile - Usually arbitrarily set at the 25th, 50th, and 75th percentiles.

++++++++++++++++++++++
Measures of Dispersion

Range - Difference between maximum and minimum value of a set.

Variance - The average of the squared distances from the mean of all data points. The sum of squared distance from the mean of each point divided by either sample size less one (n - 1) or population size. Sample variance s^2 requires the use of (n - 1) as denominator. Population variance σ^2 uses population count n. Here, n is the number of data points/elements in the sample or population.

Standard Deviation - The square root of a sets variance. The set may be a sample (s) or population (σ) standard deviation.

*Bessel's Correction:
- The use of (n - 1) as a denominator to reduce bias due to finite sample size.
σ = (Σ(X-μ)^2) / N
s = (Σ(x-x̄)^2) / (n -1)
- Part of the reason for deducting one from the sample size (n) is to increase the value of the variance. The smaller the numerator, the larger the fraction's value. This helps increase the variance for samples to reflect the fact that samples usually vary more widely than their population.

Inter-quartile Range - A basis for calculating a fence for marking outliers. The fence is often arbitrarily set to 1.5 times the inter-quartile range (IQR). IQR is the distance between the upper limit for the 25th percentile and the lower limit of the 75th percentile. Outliers are either 1.5 times the IQR above the upper limit of the 3rd quartile or lower by 1.5 times the IQR compared to the lower limit of the 1st quartile.

Z-score - The number of standard deviations from the mean to a certain value.

Coefficient of Variation - This is an indicator of how much "spread" your data has. Standard deviation divided by mean and multiplied by 100%.
C = (standard_deviation / mean) * 100%.

+++++++++++
Probability

Union - The probability of either A or B occuring. The formula for calculating unions is also the Addition Rule for Probabilities.
P(A ∪ B) = P(A) + P(B) - P(A ∩ B)

Intersection - The probability that two events/results A and B both occur, i.e. both hypotheses are simultaneously true. For independent events, multiply probability of A with that of B to get the probabilities. P(A ∩ B) = P(A) * P(B).

Addition/Additive Rule for Probabilities - The probability of A or B occuring, P(A U B), is given by: P(A U B) = P(A) + P(B) - P(A ∩ B). For mutually exclusive events, ommit `P(A ∩ B)`. P(A U B) = P(A) + P(B).

Multiplication Rule for Independent Events - If A and B are independent, P (A ∩ B) = P(A) * P(B)

Multiplication Rule for Dependent Events - Where event A is dependent on B, i.e. B -> A, P(A|B) = P(A) * P(B|A). This may be rephrased as the probability of A given that B has occurred. The formula is equivalent to: P(A|B) = P(A ∩ B) / P(B). From that formula, P(A ∩ B) = P(A|B) * P(B).

Law of Total Probability - If A is the union of a finite number of events (B1 U B2 U Bn...), P(A) = (P(A|B1) * P(B1)) + (P(A|B2) * P(B2)) +...

Expected Outcome = The expected numerical result E of an experiment for an event A. Denoted as E(A). For categorical results, E(A) = P(A) * n, where n is the number of trials performed.

Complement of an Event - The complement of an event is the mutually exclusive alternative to the event. E.g. in a coin toss, the complement of heads is tails. There is no way to get both heads and tails in a coin toss, hence the phrase "mutually exclusive alternative." P(A') = 1 - P(A) where A' is P(~A).

Disjoint Events - Events are called disjoint when they are mutually exclusive.

Sum of Probabilities for Joint Events - If A and B are joint/non-mutually-exclusive events:
P(A U B) = (P(A) + P(B)) - P(A ∩ B)
* When calculating the cumulative or sum of probabilities of joint events, make sure to avoid double-counting. Given events that are dependent on each other, P(A|B) and P(B|A), it is impossible to tell which elements of P(A|B) are also elements of P(B|A). In other words, it is impossible to determine the intersection (∩) set of joint events P(A ∩ B) using only P(A|B) and P(B|A). A and B are different sample sets.

General Product Rule for Probability
P(A ∩ B) = P(A) * P(B|A)
e.g.
Independent Coin Flips:
P(A ∩ B) = 0.5 * 0.5 = 0.25
Dependent Events:
P(A) = 0.5, P(B|A) = 0.1 (P(B) given A has happened):
P(A ∩ B) = P(0.5) * P(B|A)
Dependent Events:
P(A ∩ B) if P(A) = 0.5, P(B) = 0.1 in general, and B depends on A:
NOT ENOUGH INFORMATION, requires P(A|B) for Baye's Theorem or P(B|A) to be known.

- Conditional Probability

-- For dependent events A and B

The probability of B given A, i.e. P(B) if A must happen or is taken for granted and B depends on A:
P(B|A) = P(B ∩ A) / P(A)
- The probability of B given A is equal to the intersection (∩) of probability of B and probability of A divided by the probability of A.

The probability of A given B, i.e. P(A) happening if it depends on B:
P(A|B) = P(B ∩ A) / P(B)
- The probability of A given B is equal to the intersection of P(A) and P(B) divided by the probability of B.

* The probability of the event that must happen will be the denominator of the probability of A intersection B. To put the equation in words, the probability of A given B is the subset of P(B) where P(A) is also true. Out of the target outcomes P(B), count the number of times where P(A) is true when P(B) is true, hence P(A|B) = P(A ∪ B) / P(B)

* If P(A|B) are independent and B is given, "taken for granted," or assumed to have happened already, P(A|B) = P(A).

* In a Bernoulli Trial, there are only two possible outcomes that are mutually exclusive. It is convention to label them as success and failure.

-- For two events A and B, if the probability of A is the same as the probability of A given B, the events are independent. This is because dependent events shouldn't affect each others probability.
P(A) = P(A|B) ~ independent
P(B) = P(B|A) ~ independent
P(A) != P(A|B) ~ dependent

-- Because independent events do not affect each others probability, the intersection of independent events A and B should equal the probability of A multiplied to the probability of B.
P (A ∩ B) = P(A) * P(B) ~ independent

@@@
Bayes Theorem and Naive Bayes
Formula Variant:
P(B|A) = P(A|B)*P(B) / P(A)

- The probability of B given A is:
The probability of A given B - the subset of B where A is true, P(A|B) - multiplied by the probability of B - P(B) - all over/for every probability of A, P(A). In probability, division removes the divisor probability from the dividend probability.
- The order of probabilities in an intersection are interchangeable. P(A ∪ B) = P(B ∪ A). P(B ∪ A) can also be re-written as P(B | A)*P(A). Thus, rewriting the formula for P(A|B) can yield Baye's Theorem.
P(A | B) => P(A ∪ B) / P(B)
P(A | B) => P(B ∪ A) / P(B)
P(A | B) => (P(B|A) * P(A)) / P(B)

Bayes Theorem is equivalent to:
P(A|B) = P(A) * P(B|A) / ((P(A) * P(B|A)) + (P(A') * P(B|A')))
- The denominator term is a representation of all P(B), i.e. the subset of B where A and the subset of B where A'. The numerator represents the probability or subset of A out of B where A was true, i.e. A out of/given A where B was true. The basic principle of probability is still kept: find the ratio of target outcomes vs. all possible outcomes given your problem definition.

Naive Bayes
- In naive Bayes, the events A and B are calculated as if they are independent, i.e. A and B are naively assumed to be independent. This changes the calculation of P(A|B) to P(A) * P(B). This change changes the criteria for a success case from P(A) * P(B|A) to P(A) * P(B) * P(A). In other words, treating the different restrictions as independent increases the set of target outcomes.

@@@

- Functions of Random Variables
Given that the function expected value (μ) = b * X + a where a is a constant, and X is a random variable, μ is a linear equation. The equation standard deviation can be calculated by multiplying the absolute value of the equation's slope and the SD of X:
SD = |b| * SD(X)

++++++++++++++++++++++
Discrete Distributions

Binomial Distribution Probability Mass Function - Describes the probability of observing x successes in total after performing n trials. The term with a vertical (n x) denotes a combination of n taken x at a time, as in here: P(x;p,n)=(n x)(p)^x(1−p)(n−x). Note that n! / (x! * (n-x)!) is also the formula for calculating combinations. Specifically, it is the formula for n elements combined x at a time: nCx.

P(x) = (n! / (x! * (n-x)!)) * P^x * (1-P)^(n-x)
Here,
P(x): probability of x occurring given that it is a binomial distribution.
P: probability of x occurring during one trial, i.e the probability of success/ getting the preferred outcome.
n: number of trials.
x: desired count of outcomes.

This formula can be accessed in Excel or Google sheets using =BINOM.DIST.RANGE(). You may also use BINOM.DIST and enter false as the fourth parameter.

For binomial probability distributions, P(x) should be consistent for each trial or member event. The trials should be independent of each other.

Variance σ2 = n * P * (1-P)
Expected Value for Independent Events n to z: (n * P(n)) + (m * P(m))...(z * P(z))
Expected Value of a Binomial Probability Distribution: P(x) * n

Binomial Probability Distribution Table - This table lists the intersections between the number of trials, P, and x. It is a cumulative probability distribution. To get the probability for a specific x, take its cumulative probability and subtract the undesired outcomes. I.e, subtract the cumulative probability of the opposite "undesirable" condition. E.g.: if you need the probability of getting x >= 5, subtract the probability of getting x <= 4 from 1. If you need the probability of a specific x, say x = 5, subtract the probability of x <= 4 from the probability of x <= 5.

Poisson Distribution Probability Mass and Cumulative Mass Function - Probability of getting x successes per some continuous unit, e.g. time or distance. To get the probability of x or less/more successes, simply sum the poisson distributions of x and below or x and above. If you need the poisson cumulative mass function, sum the P(x) of all included discrete probabilties. The poisson distribution assumes uniformity of the intervals between success. If there is a reason for events or successes to occur with varying intervals - e.g. at a the end of 5 PM and during the first minutes of 8 AM - the poisson distribution cannot be used effectively.

P(x) = (λ^x * e^-λ) / x!
λ = mean or expected occurances per interval. E(X) = μ = λ = occurances / interval.
e = Euler's number.
x = Total number of occurances.

* To get the cumulative probability, sum P(x): P(x) + P(x-1) ...P(x-n) until n is zero. If you need the cumulative probability of x or more, use x + n. By convention, cumulative probability is equivalent to "less or up to the target number of successes" unless otherwise specified.

++++++++++++++++++++++++
Continuous Distributions

- Normal/Guassian/Bell-curve distribution: Maps probabilities to an area x. Probabilities cannot be mapped to a single specific outcome and have to be mapped to a range or interval instead. In a continuous normal distribution, the mean, median and mode are equal. Regardless of what the mean, median, or mode are, all normal distributions are centered and symmetric around the mean.

Standard Normal Distribution: A normal distribution with a mean of 0 and standard deviation of 1.

The area under the curve of a normal probability distribution is always equal to one. This applies to distributions where probabilities are used as the values.
Around 68% of values are within one standard deviation - i.e. one multiplied by the value of the standard deviation - less and more than the mean.
About 95% of values fall within two standard deviations from the mean in either direction/magnitude.

-- Normal Distribution Formula
f(x) = (1/sqrt(2πσ^2) * e^((-(x-μ)^2) / 2σ^2)

Where:
π = Pi, 3.14159...
σ = Standard Deviation
μ = Mean
e = Euler's number: 2.71828...

- Z-score
A z-score is the number of standard deviations from the mean for values in a normal distribution. If a data point has a z-score of 0.3, it is (0.3 * s) or (0.3 * σ) away from the mean. A z-score can be used to approximate your percentile.
Z-scores have "direction". They can be negative.
Z = (x - μ) / σ
or
Z = (x - μ) / s

-- Z-table - This table shows the percentage of values in a normal distribution that fall to the left of a score of the specified Z-value. I.e., a z-table shows the percentage of scores in a normal distribution that are lower than the value at a specified z-score. In other words, a z-table shows the percentile of a score based on the z-value.

A z-table of standard normal probabilities maps a certain z-score to the area under a normal distribution curve and to the left of the score.

++++++++++++++++++++++
Joint Random Variables

Confidence Interval - A confidence interval is a range of values with a certain probability to include an unknown parameter. E.g. a confidence interval of 10 - 20 meters with a confidence level of 90% means that a result from a certain population or sample is 90% likely to fall within 10 to 20 meters.

Alpha value - The alpha value is the probability of rejecting the null hypothesis when the null hypothesis is true. In relation to a confidence interval, the alpha value can be defined as the probability that an unknown value will not be included in the confidence interval. There are equations for setting the alpha value and some arbitrary guidelines. It may be expressed as 1 - Confidence. If the confidence for a certain interval is 50%, the probability of a value outside the interval is also 50%. The interval is usually set as the alternate hypothesis.

The universe probabilities for two sets of random variables X and Y refers to all combinations of values for X and Y.

Correlation - Correlation measures the relation between two variables. A correlation of positive or negative one indicates a perfect positive or negative linear correlation between two variables. E.g. the radius of a circle and the circumference of a circle, temperature in Fahrenheit and temperature in Celsius, amount of charitable donations and tax exemption amount. A correlation of zero means two variables are independent.

Covariance Equation:

Cov(X,Y) = Σ E((X – μ) E(Y – ν)) / n-1
Here:
X - random variable
E(X) is the expected value (the mean) of the random variable X and
E(Y) is the expected value (the mean) of the random variable Y
n - number of items in the data set.

Alternative Covariance Equation:
Cov(X, Y) = E[XY] - μx * μy
Here:
E[XY] - Expected value/average of X * Y
μx - Mean of X
μy - Mean of Y

Correlation Equation:
Corr(X, Y) = Cov(X, Y) /  (σx σy)
Here:
σx or sx - standard deviation (SD) of X.
σy or sy - SD of Y.

Related Equations:

Expected Value of X + Y:
E[X + Y] = μx + μy

Expected Value of X - Y:
E[X - Y] = μx - μy

Weighted Expected Values of the sum/difference of X and Y:
E[aX + bY] = a*μx + b*μy
E[aX - bY] = a*μx - b*μy

Variance of X + Y:
Var(X + Y) = σx^2 + σy^2 + 2Cov(X, Y)

Variance of X - Y:
σ^2 = σx^2 + σy^2 - 2Cov(X, Y)

For the variance of X +/- Y:
σx: SD of x
σy: SD of y

Weighted Variance of the sum of X and Y:
σ^2(aX + bY) = (a^2 * σx^2) + (b^2 * σy^2) + 2*a*b*Cov(X, Y)
or
Variance(aX + bY) = (a^2 * σx^2) + (b^2 * σy^2) + 2*a*b*Corr(X, Y)*σx*σy
Here:
a, b - weights.
σx - standard deviation (SD) of x.
σy - SD of y.


++++++++
Sampling

An arbitrary minimum sample count is > 30.

Selection Bias - Bias toward selecting samples more likely to react or respond to polls, surveys, and other data gathering requests or sampling techniques. 

Undercoverage Bias: omitting a segment of the population. This is usually caused by circumstance, such as phone-polling random numbers during working hours for average income. Many workers could be omitted because they cannot answer.

Self-selection bias: A bias caused by differences between people who volunteer themselves as samples compared to those who don't volunteer.

Survivorship Bias: This happens when samples that are inferior, damaged, or unobservable are removed or are not included in the sample pool.

Random Sampling - All population members have an equal chance of being selected.

Stratified Sampling - A population is sorted into segments based on certain characteristics. Members cannot belong to two or more groups. From each group, take a proportionate sample. The sample size from each group must have the same ratio to the total sample size as the group's ratio to the total population.

Clustering - The population is broken into groups and samples are taken from a random selection of groups. The group sample size ratio to total sample size must be the same as group size ratio to population size.

-- Central Limit Theorem - The means of a set of samples are normally distributed around the mean of the source population. This is proposed to be true no matter the probability distribution of the population.
The standard deviation of a sample's mean from the population mean is equal to the population SD divided by the square root of the sample size.
s(x) = σ / sqrt(n)

Here:
σ = Population standard deviation.
n = Sample size.
s(x) = Standard deviation of the sample mean from the population mean. This may be referred to as the sample standard error.

The larger the sample size, the lower the sample mean's deviation around the population mean.

- T-Distributions
These are more "conservative" estimates of probability for normal distributions. One purpose of this table is using the sample SD as a stand-in for the population SD. A t-table shows the number of standard deviations from the mean at the intersection of a significance level and degree of freedom. It may also be used with a confidence level since alpha/significance = 1-Confidence.

Z-score of a sample, i.e. Z-score for the sample mean assuming it is distributed normally around the population mean:
Z = (x - μ) / (σ / sqrt(n))
Here:
x - Sample mean.
μ - Population mean.
σ - Population SD.
n - sample size.

Confidence Interval Fourmula
CI = μ +/- ((Zα * s) / sqrt(n))
Here:
Zα - Z-score of alpha value, i.e. Z-score of percentiles for your alpha. Given a CI for 95%, take the Z-score for the 2.5th and 97.5th percentiles.
n - Sample size.
s- Sample SD.
CI - A pair of values defining a range. You will get the upper and lower values (x1 and x2) for the CI.

T-score of a sample mean/expected value, i.e. T-score for the average value of each sample member, also called the sample mean:
T = (x - μ) / (s / sqrt(n))
Here:
x - Sample mean.
μ - Population mean.
s - Sample SD.
n - sample size.

T-distribution Degrees of Freedom:
d = n - 1

* In some cases, the sample standard deviation can be used as a stand-in for the population mean. Its accuracy depends heavily on the situation, especially on factors like sample size, accuracy of measuring instruments, and the precision with which what you are measuring is created.

CI using T-score:
CI = μ +/- Tα * s / sqrt(n)
Here:
CI - A pair of values defining a range. You will get the upper and lower values (x1 and x2) for the CI.
Tα - T-score of alpha value divided by two. For a CI for 95%, take the T-score for 0.025 ((1-CI) / 2).
n - Sample size.
s- Sample SD from true/population mean.

Possible T-value calculation for those not in table:
https://www.ttable.org/

Formula for Margin based on formula for CI:
Margin = +/- (Zα * s) / sqrt(n)
Here:
Zα - Z-score of (alpha / 2). May be replaced with T-score of (alpha / 2).
s - Sample standard deviation
n - sample size

For sample proportions, n = 1.
Margin = +/- (Zα * s)
or
Margin = +/- (Zα * sqrt((P*(1-P)) / n)
P - sample proportion with desired characteristic, i.e. target proportion.

This is confirmed by a Lumen Learning course:
EBP = Z(α/2) * sqrt(pq/n)
Here:
Z(α/2) - Z-score of alpha/2.
p - the estimated proportion of successes.
q - 1-p where p is a sample proportion.
n - sample size.
https://courses.lumenlearning.com/introstats1/chapter/a-population-proportion/

The standard deviation of a sample proportion is calculated using:
s(proportion) = sqrt((P * (1 - P)) / n)
Here:
P - proportion that meet target value or hypothesis, i.e. proportion of sample that contains target characteristics.
When calculating the standard deviation of a sample proportion, use the population percentage when you can.

A proportion can be given a Z score.
Z(prop) = (Prop - P) / s
Here:
Prop - Proportion in sample, results of experiment.
P - Expected value, hypothesis, target or proposed proportion.
s - Proportion standard deviation.

- T-test variants

-- T-test for two independent samples with unequal variance. Sample sizes may or may not be equal.
T-score = abs(x1 - x2) / sqrt((s1^2 / n1) + (s2^2 / n2))

The degrees of freedom for samples with unequal variances:
df = ((s1^2 / n1) + (s2^2 / n2))^2 / ((1/n1 - 1)*(s1^2 / n1)^2 +  (1/n2 - 1)*(s2^2 / n2)^2)

If the samples have the same standard deviation/variance:
df = n1 + n2 - 2

++++++++++++++++++
Hypothesis Testing

Null Hypothesis - Default hypothesis. Assumed hypothesis. It often assumes no significant change or difference between experiment groups. Technically, it states that no statistical relationship and significance exists in a set of given, single, observed variables between two sets of observed data and measured phenomena.

Alternate Hypothesis - It's probability is the complement of the null hypothesis. This hypothesis is the theory that needs to be proven.

Type I Error - Rejecting the H0 when H0 is true. The probability of a type one error is referred to as the "level of significance" or "alpha."

Type II Error - Failing to reject the H0 when H0 is false.

Analysis of Variance (ANOVA)
- Analysis of Variance is used to check if there is a significant difference between two or more samples. ANOVA is used to avoid the increased risk of type I error caused by the stacking alpha probabilities. BINOM.DIST.RANGE(3, 0.5, 1, 3) = 14.263%, i.e. confidence ~ 85.7%. Although applicable when comparing two samples, ANOVA is appropriate for three or more groups since normal significance testing can be used on two samples. The F distribution used to set values in ANOVA is skewed to the right. This is probably because ANOVA calculations result in negative differences increasing the F-value or F-score. Both negative and positive differences will increase the F-score. 

F-value - An F-value is the ratio between the variance among groups relative to the average group mean. Part of this calculation is Σ(x - x̄)^2, a.k.a sum of squares.

F = (SSg/dfg) / (SSe/dfe)
Where:
SSg = Sum of squared differences between each group mean and the average mean of the groups.
	Take the sum of all samples' variance from the mean group variance. Multiply the sum to the number of values/rows in each group.
	(Σ(Xg - x̄g)^2) * sampleSzie.
	Xg is the group mean.
	x̄ is the mean deviation of each group from the average group mean, i.e. (Xg1 + Xg2 +...)/NumberOfGroups. 
dfg = Degrees of freedom for the set of groups. Number of groups less one.
SSe = SS within groups. For each group, take the variance of its values. After calculating each group/sample's variance, sum the variances.
dfe = Degrees of freedom for the error. (SampleSize - 1) * NumberOfSamples.

* This assumes all groups have the same number of samples.

The group degrees of freedom and error degrees of freedom will be used to find the critical score/F-value for the F-distribution. 

Two-way ANOVA
- A two-way ANOVA calculates if there is a significant difference among samples if their values are grouped or blocked according to another independent variable. E.g. Travel time per route, and travel time per day of the week. A two-way ANOVA determines if there is a significant difference between travel time samples/groups depending on the day of the week.

F = (SSg/dfg) / (SSe/dfe)
Where:
SSg = Sum of squared differences between the average mean of the groups and each group mean. Same as one-way ANOVA.
dfg = Degrees of freedom for the set of samples/groups. Number of groups less one. Same as one-way ANOVA.
SSe = Sum of Squares Error. This is calculated differently for two-way ANOVA:
	SSe = SSt - SSg - SSb
	Where:
	SSb = (Σ(Xblock - x̄block)^2) * DataPointsPerBlock where x̄block is the mean of the block averages and Xblock is the block average.
	SSt = Σ(x - x̄block)^2 where x is an individual value in a sample.
dfe = Calculated using (NumberBlocksOrRows  - 1) * (NumberSamples - 1)

The critical F value (F-critical) is calculated separately for groups and blocks.

Two-way ANOVA with Replication
- Two-way ANOVA with replication is the method used in most real problems. It allows multiple samples within a block, not just multiple blocks within groups.

1. Calculate the mean for each sample and block intersection, i.e. get the mean of data points/rows for each block within each sample. 
2. Calculate group means, i.e. column means, and block means. Block means include all data points in a block regardless of sample.
3. Calculate the overall mean. You may get the mean of the samples or get the mean of all data points.
4. Calculate the block Sum of Squares:
	SSb = (Σ(Xblock - x̄block)^2) * DataPointsPerBlock
	Where x̄block is the mean of the block averages and Xblock is the block average.
5. Get the Sum of Squares for columns.
	SSg = (Σ(Xgroup - x̄group)^2) * DataPointsPerGroup
	Where Xgroup is a column/group mean and x̄group is the mean value of group averages.
6. Get the degrees of freedom for columns:
	dfg = n - 1, where n is the number of data points per group.
7. Get the Sum of Squares Interactions. For each group-block-intersection mean, subtract the corresponding block and column means. Add the overall mean and then square the result.
	SSi = Σ(x̄gb - (x̄b + x̄g) + x̄)^2
	Where:
	x̄gb = Mean for a group and block intersection.
	x̄b = Mean of the group-block intersection's corresponding block.
	x̄g = Mean of the group-block intersection's corresponding group/sample.
	x̄ = Overall mean.
8. Calculate the Sum of Squares for the Total.
	SSt = Σ(x - x̄)^2, where x is a data point and x̄ is the mean for all data points. 
9. Get the SSe.
	SSe = SSt - (SSb + SSg + SSi)
10. Get the degrees of freedom for error.
	dfe = NumberOfBlocks * NumberOfGroups * (n - 1), where n is the number of samples for each group and block intersection.

F = (SSg/dfg) / (SSe/dfe)

Chi-square Test
- The Chi-square test is used to find "goodness of fit" and to check if observed results are deviating significantly from expected results. It is used to determine the probability of observing a specific frequency of certain events. The Chi-square test assumes that the outcomes and future outcomes are independent and mutually exclusive.

For every possible category to be observed, take the squared difference between the expected outcome and the observed outcome. Divide the squared difference by the expected outcome. Last, get the sum of the per-variable calculations.
x^2 = Σ((O - E)^2 / E)
Where:
E = Expected value or outcome count.
O = Observed value or outcome count.
Chi-square degrees of freedom = CountOfPossibleObservationCategories - 1

- Without an explicit or proven expected number of outcomes per event, assume that the probability of each observation is uniformly distributed. For six possible outcomes, the expected value would be the total observations divided by six.

++++++++++++++++++++++++
Simple Linear Regression

- A line made to fit a data set with a linear relationship always follows this equation:
y = b + mx + c
b: Y-intercept. A point of the line that crosses the y-axis of the graph. The value of y when x = 0.
m: Slope or rise. Change in Y per change in x, Δy / Δx.
c: A constant added in to adjust for error. Error coefficient.

Residual - This refers to the difference between the predicted value (y) of an input (x) versus the actual result of x (y1).
R - This symbol represents the proportion of change or variation in y (dependent variable) explained by the variable x (independent/driving variable). In other words, R is the correlation between the two variables.

R-squared and Adjusted R-squared - R here is not a residual. It is the percentage of the variance in the y variable explained/caused by the equation predictor or independent variables. The adjusted R-squared compensates for the number of variables. More variables increases the R and R-squared regardless of predictive power. Thus, a large difference between the R-squared and adjusted R-squared values is a sign of excess variables.

Standard Error - This is the amount by which a sample mean will deviate around the source population mean. If the population mean is not provided, the sample mean may be used as a stand in but confidence and alpha must be calculated using a T-table.

T-stat - The T-stat is the ratio of the coefficient to the standard error of the equation when the coefficient is used as the only predictor.

When using linear regression to do a "point estimate" or estimate on one sample instance, make sure your data point is within the maximum and minimum of the training set. I.e, make sure your dependent variable value is within the range of the reference data set.

- Confidence Intervals for Predicted Changes in Y
This is the confidence interval for a range of possible changes in Y compared to a certain reference value of a predictor.

CI = Δy +|- T * Δx * (SE)
Δy: Predicted change in value/output of the equation given Δx from x.
T: T-stat. The number of standard errors from zero to the coefficient/slope.
Δx: The amount of change to x from an arbitrary reference value.
SE: Standard error of the predictor variable.

- Standardized Coefficients/Beta Weights
The amount of standard deviations the dependent variable will change due to one standard deviation of change in the predictor variable. Each variable's respective standard deviation is used as a measure. According to StatisticsHowTo, it isn’t actually the coefficients that get standardized, but the variables. Betas are calculated by subtracting the mean from the variable and dividing by its standard deviation. This results in standardized variables having a mean of zero and a standard deviation of 1.
https://www.statisticshowto.com/standardized-beta-coefficient/

P-value - A variables p-value is a decimal number expressing the chance of observing such a result assuming the null hypothesis is true, i.e. assuming that the variable is not statistically significant. In linear regression, the p-value is the chance that the variable coefficient is non-zero due to random chance. The variable does not have predictive power and has a true coefficient of zero. If your p-value is greater than your alpha or "critical value", the data does not pass your confidence level for the specific variable coefficient.

- F-statistic
An f-statistic tells the predictive value the equation predictors have on the result. It is the joint significance to the result for a group of variables. The "significance of f" is similar to the p-value. The significance F is the probability that the null hypothesis in our regression model cannot be rejected, i.e., it indicates the probability that all the coefficients in our regression output are actually zero.

- Standard Error for the Prediction of a Single Observation (Y)
SE = Si * sqrt(1 + (1/n) + ((x - x̄)² / (s² * (n - 1))))
SE = Si * sqrt(1 + (1/n) + ((x - x̄)^2 / (s^2 * (n - 1))))
Si: Standard error of the equation. This is also s/sqrt(n) or σ/sqrt(n).
n: Sample size, number of data points.
x: Value of the predictor variable used in calculating the output y.
x̄: Sample mean for the predictor variable x.
s: Sample standard deviation for the predictor variable.

- Confidence Interval Around a Prediction of a Single Observation(Y):
CI = y +|- T * SE

The standard error of a single observation should be higher than the standard error of the equation.

The value of T is set by your degrees of freedom (n - # of coefficients) and alpha. An experiment with ten items and required confidence of 99% would require a T for 0.005 (for two-tailed tests) or T for 0.01 (for one tailed tests where a lower or upper limit is in the accepted range/excluded from alpha) given eight degrees of freedom.

The confidence interval assumes the prediction as the most likely value. It is the value around which the true measurable results of the predictor variable are normally distributed. If graphed, the prediction would be the center of the normal distribution "bell." Around it, on the x-axis, would be other potential values. Their probabilities would form the dumbbell curve or shape.

The T-value or count of standard deviations is the multiplier for the standard error. Assuming the prediction is correct and given a standard error observed between the equation predictions and actual results, T * SE would give the amount of deviation around the prediction. The T-value also represents confidence or the proportion of possible values for y that your confidence interval includes.

- Standard Error for the Expected Value of Y at X:
SE = Si * sqrt((1/n) + ((x - x̄)² / (s² * (n - 1))))
SE = Si * sqrt((1/n) + ((x - x̄)^2 / (s^2 * (n - 1))))

The expected value of Y is synonymous to "mean of Y" in this case. E(Y) is the average value of Y given a certain value of X.

@@@
Assumptions of Linear Regression

Linearity - There is a somewhat directly proportional and continuous relationship between the input and output variables.

Independence of Errors - The error between the predicted and actual values (residuals) should be independent. If the residual changes reliably based on previous residuals or at predictable intervals, the regression will be less accurate.

Homoscedasticity - There is no trend in the size/magnitude of the residuals. One way to test this is to take the square of the residuals. If they increase in a mostly linear way, then there is a trend in the residuals and the data is NOT homoscedastic.

Normality of Error Distribution - The actual values of the observations should be clustered around the best-fit line. Residuals with very high absolute values should be rare.

* Visualizing your variables can be used as a shortcut or guide to check if you can perform a valid linear regression.

+++++++++++++++++++
Multiple Regression

- Dummy Variables
Question:
I have questions about how the coefficient of each dummy variable is calculated and about multicollinearity.
To begin, I'll use the restaurant problem from the single linear regression problem set.
It is given that prediction values (y) using the entree coefficient (x) deviate by 0.111 (standard error, SE) on average from the correct value.
It's also given that the coefficient or optimized multiplier of the entree value is 0.625. This is the multiplier with the lowest equation SE.
This are my assumptions:
If the true coefficient of entree price is zero, the calculated coefficient will deviate around zero by the same amount as the equation SE.
If entree price has no effect on alcohol sales, i.e. entree price' true coefficient is zero, its coefficient should be less than or equal to the SE.
I assume that if x' true coefficient were zero, optimizing the coefficient of x would bring its coefficient close to the SE.
There will always be unexplained or random errors in prediction values. Samples are imperfect. There could be hidden variables.
An insignificant variable could get a coefficient close to the standard error since unexplained deviations are "blamed" or attributed to it.

Ans:
*** The SE in linear regression is basically the standard deviation of the errors in the model.  In other words, a better way to say this might be that, if the coefficient of the predictor variable is zero, then the coefficient is likely to be within ~2 standard errors of zero. An actually insignificant variable could appear significant because of problems with the sample or the selection of variables to run in the regression.  Yes.

1. When using dummy values to represent a nominal variable, wouldn't the excluded encoding "transfer" its value to the included variables?
Won't the effect of the missing dummy value be attributed to the value that happens to be encoded as 1?
I'll use the example categories of customer referral source. Let's say the referrals by a friend have the highest positive effect on spending.
Removing referrals from a friend would decrease consumer spending (y).
Wouldn't regression mistakenly assign higher values to the encoded dummy value to compensate?
Those customer data points who were referred by a friend simply have zeros for all four binary variables.

2. I don't understand how to get the effect of the dummy variable relative to the omitted variable.
Is the regression equation done behind the scenes using the omitted value?

*** [For 1 and 2] No. Every customer data point will be counted. Those customer data points who were referred by a friend simply have zeros for all four binary variables.

3. How does making a dummy value for all categorical values cause multicolinearity? I didn't get how the equation automatically factors in the value that is not encoded.

*** Honestly, I see people make n dummy variables for n categories all the time, so you wouldn't be alone in not using n-1 categories.  The problem with this is that your results will have no point of reference and the additional variable will carry no additional information relative to the others.  This is a common mistake.

Follow-up Questions:
Where would the effect or coefficient of friend referrals be "attributed" to if "referred by a friend" is the default value? It would not be mistakenly transferred to another referral method since other dummy variables are all zeroed.

Next, is there a way for me to find the effectiveness of friend referrals in contributing to sales if it is the default value? I believe its relative effectiveness is quantified against the encoded referral methods' coefficients. If TV had a 0.2 coefficient, I would've said TV only has 0.2 friend referrals contribution to revenue. Maybe that "relative effectiveness" could even be checked for significance vs other referral methods. How do I quantify the contribution of an individual referral method vs. other variables in the regression, like gender or customer salary?

*** When you are using dummy variables, the effect of any one of them will always be relative to the other dummy variables that refer to the same attribute. Now, if you are interested in assigning an exact value to the effect of one of those variables relative to all others, you will need to code a binary dummy variable for just that attribute.  I.e. you could code Friend Referral as 1 and all other source types as 0.

Question:
I want to know how linear regression avoids assigning the coefficient of the default dummy variable to other variables in the regression instead of the encoded dummy variables. My assumption is that assigning the "effect" or coefficient to the wrong variables would raise the error of the regression model. Thru some optimization method, the regression will eventually assign or "attribute" the relative effect of the default value to its corresponding encoded values. The problem with my guess is that the model could simply force-fit the coefficient of the default dummy variable to other non-related variables. Multiplication has the same effect no matter the order, so the coefficient of the default dummy variable could be adjusted and transferred.

*** In a multiple regression, every independent variable that you include in the regression will have an effect on every other one,  which is usually the point of including more than one variable. In effect, you want to account for the effect of the variables you really care about in the context of other key factors. In the example you gave where you have variables for customer salary and each of the referral sources, you would be able to measure the effect of referral sources controlling for customer salary.  For instance, this would address the issue of some sources having higher average customer salaries than others.
To summarize, every variable included in a multiple regression will alter the coefficients of the others in some way unless they are completely independent.  Dummy variables are no different in this way.  And that's definitely a feature of regression, not a bug.

- Time-lagged Variables

+++++++++++++
Miscellaneuos

@@@@@
Setup

- Install latest version of Python. Ubuntu version usually lags behind latest stable version. Download, unzip, and process latest tar file.
Source:
https://phoenixnap.com/kb/how-to-install-python-3-ubuntu

- Create a virtual environment. Do not do anything past creating an environment.
https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-20-04

- Install helper modules, especially pip.

- To use an environment, navigate to the environments folder and enter this command:
source ./bin/activate

