++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using some input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

- To start Spyder in Ubuntu Linux:
conda activate spyder-env
spyder

Here, "spyder-env" is the name of your Conda environment. This is the default name and it may be changed.

- A plugin is needed to open and edit notebooks in Spyder:
conda install spyder-notebook -c conda-forge

+++++++++++++++++
Linear Regression

For a cost function J with parameters w and b, J(w, b), continuously update w and b to minimize J.
w = w - α (∂/∂w)J(w,b)
and
b = b - α (∂/∂b)J(w,b)
Where:
α = Learning rate. This is a decimal value between 0 and 1. It functions as a multiplier to the amount of change.
(d/dw)J(w,b) = This entire term represents the value of the partial derivative of the function J with parameters w and b. The term "d" indicates to change the value of w when looking for the derivative and to keep other values constant.

* A derivative is the rate of change of a function relative to a variable. A derivative may be noted as f' or as d/dx. A partial derivative may be noted as ∂/∂x or f. The term x can be any variable. Calculating a derivative involves finding the slope of a point in a line using the properties of tangents, limits, and triangle trigonometry.
* The update operations of w and b have to be done at the same time and in step. When performing the descent, the current/old values should be used as input to J. The updated values will only be used in the next iteration.

+++++++++++++++++++
Logistic Regression

Sigmoid Function
- a.k.a Logistic Function. It outputs values between zero and one given z.
g(z) = 1 / (1 + e^-z)

- A linear or polynomial function is the input (z) to a logistic/sigmoid function. The result of a logistic function is the probability of a positive result, i.e. the sigmoid function gives the probability that the target characteristic is present for the sample/vector x.
f(w, b) = P(y = 1 | x; w, b)

Decision Boundary
- The decision boundary is the value at which the probability between g(z) = 1 and g(z) = 0 is equal or neutral. g(w,x) = w1x1 +...wnxm = 0. The decision boundary may be calculated by substituting values into the input equation and then solving for or simplifying them. 

Logarithm
- A logarithm is the exponent or power to which a base must be raised to yield a given number. E.g. the logarithm of 8 with base 2 is 3. Two must be raised to the exponent three to yield eight.

Simplified Cost Function for Logistic Regression
- Refer to Google Drive notes. This function is convex (only has one global minimum) and uses the statistical principle of maximum likelihood.

+++++++++++++++++++++++++++++++++++++++
Gradient Descent with Linear Regression

These are the expanded formulas for a cost function used in single-variable linear regression. Here, the function f with parameters w and b - f(w,b) - is a linear function.
w = w - α * (1/m) * Σ(f<-w,b(wx^i + b) - y^i) x^i
b = b - α * (1/m) * Σ(f<-w,b(wx^i + b) - y^i)

* Due to the rules of calculus, the x^i term happens to be removed when calculating the update to b.

- A property of gradient descent cost functions for linear regression is that they do not have multiple local minima. This kind of function is called a convex function, i.e. a function with only one global minimum.

Convergence Checking
- Check if the error or cost function decreases by a certain amount, ε. Compare the error optimization of different learning rates (α) over a small amount of runs (e.g. 30) and gradually change α.

Feature Engineering for Regression
- Features may be removed automatically by the model. Another way is to use domain knowledge to transform or combine data into new features and compare their performance to a certain baseline or "control" set of features. You may also compare the effect of new or aggregated features to the separate source features.

Polynomial Regression
- You may derive a new feature from x by taking its various powers - usually the second or third - or taking x' square root. This may not be applicable to some features with negative values. For negative values, it may be useful to transform the absolute value and then multiply it to a negative value as part of the transformation process.

Loss Functions and Cost Functions
A loss function is calculated per set of parameters and input. This is equivalent to calculating using the data of one row. A cost function uses the principles of derivatives and polynomials to find the slope of the entire data set. The cost function uses the average of all results in the data set to determine the general direction parameters need to be shifted.

+++++++++++++++
Feature Scaling

Z-score normalization - Use the z-scores as x-values.
x = (x - μ) / σ

Min-max normalization
x = x / (max(vectorX))

Mean normalization
x = (x - μ) / (max(vectorX) - min(vectorX))

- If certain features have very large or very small x-values, the gradient descent model may take longer to reach a low-value cost function. The user has low control over the amount of change to a parameter in the early stages of the cost function optimization. Values with an uneven scale may force the model to "bounce" around large ranges and take longer to minimize error.

- When normalizing, it is recommended to have values within a small countable negative and positive range to retain magnitude and direction versus some reference point.

++++++++++++++++++++++++++++
Overfitting and Underfitting

- If a model fails to capture or reflect the relationships between parameters and input strongly enough, its predictions will not change properly with respect to new or real-world input. This is underfit model is said to have high bias, similar to having a "preconception" and ignoring data that goes against the preconception.

- When a model is overfitted, it responds too strongly and too specifically to training data. Due to the mathematics of the model, any slight deviation of the input from the training data will cause very inaccurate predictions. Slightly different data sets can give very different results, so an overfit model is said to have high variance.

- A good model adjusts predictions based on input without giving erroneous results when presented real-world data or proper data from the same population. This model is a "generalized" model.

- Overfit may be caused by excess parameters that hint at relationships which don't exist in the population. Excess parameters may also cause underfit due to noise. The model could miss the proper cost function minimization parameters since it does not have good feedback.

@@@
Avoiding or Solving Overfitting

- Collect more samples. Make sure as many samples as possible are representative of your population.

- Remove some features. A disadvantage of reducing features is that some nuance may be removed. Testing and other kinds of analysis are needed to remove features properly.

- Regularization. Reduce certain or all feature values to lessen their effect on the chosen parameters. The reduction calculation has to be the same for all features. A regularization parameter (λ) has to be chosen.
The following term may be added to the simplified logistic regression cost function to regularize the input.

λ/2m * Σ(j=1 n) w[j]^2
Where:
n = Number of features.
j = Index for the row/horizontal vector. Index j points to a feature parameter.
m = Number of rows/horizontal vectors.
w = Input or feature value.

+++++++++++++++++++++++++++++++++++
Matrix Operations and Vectorization

- The dot product of two vectors x and y is equivalent to the dot product of transpose(x) and y.

- You may only perform a dot product on vectors with the same length.

@@@
Matrix Multiplication Rules

- To multiply matrices, the first matrix must have the same number of columns (vertical vectors) as the other matrix' rows. E.g. three by two and two by four. To multiply a twelve by n and m by twelve matrix, the m by twelve matrix has to be the first input of the operation. This is a side-effect of the dot-product rule for multiplying vectors.

- The result of a matrix dot product will have the same number of rows as the first matrix and the same number of columns as the second.

- In general, matrix multiplication is NOT commutative. The order of the matrices is significant in matrix multiplication. Only certain edge cases result in commutative matrix multiplications.

++++++++++++++++++++++++++++++++
Basic Artificial Neural Networks

@@@
Activation Functions

- For binary classification problems, the sigmoid function is usually best for the final or "output" layer. The network will predict the probability of y = 1 or 0. The decision boundary (values where P(y=0) = P(y=1)) or an arbitrary probability may be used as a threshold for activation.

- Problems that fit the criteria of linear regression often need the linear activation function for the output layer. It is recommended to avoid using the linear activation function for hidden layers. If a linear or sigmoid function is used at the output layer, the results will be similar to ordinary linear or logistic regression.

- If the ground truth or source of truth (y) should only have positive values, use the Rectified Linear Unit (ReLU) function for the output layer.

*Note: The activation function of the output layer is not always the same as the hidden layer/s. The most common hidden layer activation function is the ReLU.

ReLU - g(z) = max(0, z). This function is slightly faster to compute than the sigmoid function. More important for speed, the sigmoid function has two areas where the output is flat when graphed. When using gradient descent, the computations for those values will change slowly. The ReLU only has one area where it is flat. If done properly, the ReLU will be computed faster without losing accuracy.

- SoftMax
Soft Max Loss Function = -log a(j) if y = j

The Softmax activation function computes the probability of each label versus the cumulative probability of all labels. For each output layer neuron or unit output a, the Softmax function g is calculated using the probabilities of each class label.
a1 = g(z1,z2,...zn)

Thus:
g(z1) = (e^z1) / (e^z1) + (e^z2) +...(e^zn)
...
g(zn) = (e^zn) / (e^z1) +...(e^zn)

@@@
Convolutional Neural Network
- A Convolutional Nueral Network (CNN) has units or neurons that only process part of the previous layer's output. These units will only process a section of the input data. It can compute results faster and reduce overfitting. The data or input for each neuron may overlap with other neurons. For images, it could mean overlapping coverage of an area. For time-series data, the time boundaries of the neurons may overlap. There, the next neuron has some idea of earlier or later data.

ADAM
- Adaptive Moment Estimation changes the learning rate for each parameter. It also adjusts the learning rate based on the direction and amount of change in the computed adjustment. As of July 06, 2023, A. Ng. described ADAM as the defacto optimization algorithm for neural networks.

- Back-propagation and Forward-propagation
Back-propagation using a computational graph takes N(nodes) + P(parameters) resources versus N * P during forward prop.
My understanding is that instead of calculating the derivative of J with respect to each parameter, the derivative is calculated at the end for one parameter. That derivative is used to calculate the derivative of other parameters without substituting all other parameters and x-values.

++++++++++++++++
Model Evalutaion

- Divide your training data into two representative sets. One will be used to train and the other will stand in for unknown test data. The convention is to divide train and test sets seventy to thirty. Extreme differences between the model error during training and the model error processing the test function is a sign of overfitting.

- The lowest test set error is not always a sign of the best parameters for a machine learning algorithm. Even if a test set error is low, it is still possible that the model happened to overfit to the test set. One solution is to add a third set to measure accuracy and pick a model before evaluating using the test set. The convention is 60-20-20. The cross-validation set is also called the development set.

@@@
Performance Baselines, Bias, and Variance

- If possible, benchmark model performance against human performance.

- Competing or standard algorithms may also be used as a baseline for performance, variance, and bias. The same training, cross-validation, and test data must be used for all models. To get the average train-test or train-cross-validation error difference, several different splits of data will have to be used. Process the average error difference.

- The training error is expected to increase as more data is used, eventually resembling a straight line. The cross-validation error is expected to decrease as training data increases. The graph of the cross-validation error will also straigten out as it approaches zero. Steadily increasing the train data with different sizes of subsets is usually too resource-intensive to be practical.

- Large neural networks are usually low-bias machines. Increasing the number of layers or units both reduce bias. For certain reasons, regularization can prevent large neural networks getting high variance.
