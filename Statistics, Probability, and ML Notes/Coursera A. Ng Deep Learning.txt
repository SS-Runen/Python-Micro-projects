++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using sample input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

- Numpy arrays and Pandas data frames list row count and "column" count in their shape like so: (n_rows, n_cols). Technically these are just nested arrays with two dimensions. Flattening them would show n_rows as elements. Each element holds n_cols sub-elements.

- To test if values meet conditions in Python, use the assert() method. E.g. assert(Z.shape[0] == w.shape

+++++++++++++++++++++
Neural Network Basics

References for Derivative Rules:
https://www.mathsisfun.com/calculus/derivatives-rules.html
https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/economics/differentiation-and-integration/rules-of-differentiation.html
https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf

Reference with some proof examples:
https://www.sfu.ca/math-coursenotes/Math%20157%20Course%20Notes/sec_DerivativeRules.html

- The derivative of the cumulative cost function J(vector(w), vector(x)) is the average loss for the training set. Because of that, the derivative of the cumulative cost function with respect to a parameter w is the average of derivatives for the per-item loss function L(p̂ - p) where p̂ is the prediction and p is the correct value or label.

- The Sigmoid and TanH (Hyperbolic Tangent) functions have a slope that is close to zero whenever Z (the resulting value of the parameter and input value polynomial) is very large or very small. This means gradient descent proceeds slowly for very large or very small values. The TanH function is a shifted version of the Sigmoid function that allows values between -1 and 1. The ReLU function performs just as well or better than Sigmoid or TanH functions in hidden and non-output layers, but is significantly faster to calculate. It is common to have the final output layer use the Sigmoid function for binary or multi-class functions and another activation function for the heady layers. The ReLU function (max(0, z)) does have the disadvantage of losing some information due to treating negative Z values as zero. This problem is solved by the leaky ReLU function: max(c*z, z) where c is an arbitrary constant that uses a proportion of z as the lower limit, e.g 0.001 * z. If z is negative, a small negative proportion of z is the maximum and thus the output of Leaky ReLU.

- Linear or identity activation functions would make adding neural network layers pointless. The composition of a series or sequence of functions is no different from ordinary non-neural-net linear regression. Linear and identity functions are typically only used at the output layer.

- The parameters of each neural network unit or neuron have to be randomized and diverse to avoid identical outputs. Setting the bias to zero or only one value is fine, but the hidden units or neurons should have significantly different initial parameter values.

- The weights should be less than one and are typically very small, e.g. 0.01. This is done to avoid extreme variations and to prevent very small derivatives. Many activation functions have very small derivatives given large negative or positive output. There are certain cases where large weights are good or acceptable.

- There are some neural network functions or formulas that take exponentially more hidden units to compute compared to less hidden units organized into more layers. That is one reason deep neural networks are used in place of shallow networks with many hidden units. This is related to Circuit Theory and the principles of convolutional neural networks.

@@@
Derivatives of TanH, Sigmoid, ReLU, and Leaky ReLU Activations

Sigmoid:
g(z) = 1 / (1+e^-z)
g'(z) = g(z) * (1 - g(z))
g'(z) = a * (1 - a) where a is the output of the activation function g(z) and z is the result of W * X.

TanH:
g(z) = (e^z - e^-z) / (e^z + e^-z)
g'(z) = 1 - (tanh(z))^2
g'(z) = 1 = a^2

ReLU
g(z) = max(0, z)
g'(z) = 0 if z < 0, 1 if z >= 0
*Mathematically, g'(z) if z = 0 is undefined, but it may be treated as zero for neural networks.

Leaky ReLU
g(z) = max(c*z, z)
g'(z) = c if z < 0, 1 if z >= 0

@@@
Working with Numpy Arrays

- When working with Numpy ND-arrays, it is common to place each example or sample as a visual "column" with each row or item in the column holding a feature value. Each "row" holds different samples' values for one feature X. Assume X is an n_rows x m_columns matrix with n rows corresponding to n samples and m columns for m features. Numpy and Pandas present that in the correct visual way by default. When multiplying an (n_example,m_feature) matrix X to a parameter array W with m parameters corresponding to m features, W has to be transposed into a single-column vector for the numpy.dot() function to work. This is consistent with the rules of matrices in mathematics. In mathematics, you take the dot-product by multiplying each row of the first matrix - in this case X - element-wise to the corresponding column of the second.

* In Numpy, a (1, ) matrix is sometimes treated as if it could be both a (, 1) or (1,) matrix. Thus it is recommended by this course to use a (1,n) or (n, 1) matrix, avoiding all matrices with an undefined rank.

- When using Numpy, it is common to use an n_rows by m_column matrix X with each row corresponding to one feature, not one sample. Each column holds multiple features for a single sample. Assume a parameter array W holds n_rows elements. Use W as the first matrix. Take the first and only row of W and multiply each element w to the corresponding column, i.e. feature value. Repeat the process for each column of X. Also, matrix multiplication is not commutative. In this case, Numpy and matrix multiplication rules require that X be transposed if it is the first matrix.

- Multiplying an (n1_row, m1_column) matrix by (n2_row, m2_column) matrix should yield a matrix with (n1_row, m2_column) rows and columns. In matrix multiplication, first process matrix one row one. Take the dot product of matrix one, row one with each column of row two. Place each number corresponding to a dot_product(row1, column_m) result in a new column. Move on to row two. Place the dot product results for row two on the next row of the result matrix. References:
https://www.mathsisfun.com/algebra/matrix-multiplying.html
https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-matrices/a/multiplying-matrices

@@@
Regularization

Logistic Regression
- When applying the calculations to find the derivative of a parameter, the regularization term will be removed from the update amount. The regularization term here reduces the effect of very large negative or positive polynomial/parameter-input product/z values. It penalizes large parameters and input values. For certain activation functions like the TanH and Sigmoid, smaller values are also better than large negative or positive values. Gradient descent is executed faster for some activation functions if the activation values are lower.

- For the logistic regression cost function, this is the regularization term:
λ/2m * Σ(j=1 n) w[j]^2
Where:
n = Number of features.
j = Index for the row/horizontal vector. Index j points to a feature parameter.
m = Number of rows/horizontal vectors.
w = Input or feature value.
* This is taken from notes on logistic regression.
This regularization is also referred to as L2 regularization and, in matrix mathematics, is also equivalent to w.transpose() * w.

- For logistic regression using neural networks, this is the regularization term: 
+ λ/2m * ||w||^2
or, expanding ||w||^2:
+ λ/2m * Σ(i=1 n_layer)Σ(j=1 n_layer-1)(w(i,j l)^2
where:
n_layer denotes w of the current layer.
n_layer-1 denotes a parameter or variable of the previous layer.
j = Number of neurons in the previous layer.
i = Number of neurons in the current layer.
* The matrix norm of a neural network is called the Frobenius norm.

- The bias (b) may also be regularized, but its effect is negligible in practice if there are many different values for the parameter array w.

- If L1 regularization is used instead, w will be a sparse vector, i.e. it will have many zeroes. Replacing 2m with m makes the regularization value higher and more likely to cancel out some values.
+ λ/m * ||w||^2

Dropout Regularization
- Give each unit or neuron a certain chance to be eliminated. If a neuron is selected for elimination, set its activation output to zero. Later, divide each activation value by the probability of keeping a unit. This keeps the scale of the activation values consistent and keeps the expected value of A constant. When running a prediction or test, do NOT implement dropout. The keep probability can vary per layer. In practice though, zeroing out feature input values is rarely done.

- Dropout regularization forces the network to adjust weights based on a more diverse set of parameters and activation values. Since certain values are randomly zeroed, the network will eventually accumulate adjustments based on certain features while ignoring others. This reduces overfitting/high variance. Dropout regularization shrinks the weights.

- A disadvantage of dropout regularization is that the cost function is not well-defined and difficult to track. Due to the randomness of changes, the changes in J could be random and hard to plot. This can be solved by turning off dropout for debugging or descent checking.


