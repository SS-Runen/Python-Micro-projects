++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using sample input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

- Numpy arrays and Pandas data frames list row count and "column" count in their shape like so: (n_rows, n_cols). Technically these are just nested arrays with two dimensions. Flattening them would show n_rows as elements. Each element holds n_cols sub-elements.

- To test if values meet conditions in Python, use the assert() method. E.g. assert(Z.shape[0] == w.shape) or assert Z.shape[1] == W.T.shape[1], "Z column count mismatch with W.transpose column count."

- Test validation and logic-checking are mandatory. For complicated functions or classes, more granular or step-by-step checks may be necessary.

+++++++++++++++++++++
Neural Network Basics

References for Derivative Rules:
https://www.mathsisfun.com/calculus/derivatives-rules.html
https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/economics/differentiation-and-integration/rules-of-differentiation.html
https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf

Reference with some proof examples:
https://www.sfu.ca/math-coursenotes/Math%20157%20Course%20Notes/sec_DerivativeRules.html

- The derivative of the cumulative cost function J(vector(w), vector(x)) is the average loss for the training set. Because of that, the derivative of the cumulative cost function with respect to a parameter w is the average of derivatives for the per-item loss function L(p̂ - p) where p̂ is the prediction and p is the correct value or label.

- The Sigmoid and TanH (Hyperbolic Tangent) functions have a slope that is close to zero whenever Z (the resulting value of the parameter and input value polynomial) is very large or very small. This means gradient descent proceeds slowly for very large or very small values. The TanH function is a shifted version of the Sigmoid function that allows values between -1 and 1. The ReLU function performs just as well or better than Sigmoid or TanH functions in hidden and non-output layers, but is significantly faster to calculate. It is common to have the final output layer use the Sigmoid function for binary or multi-class functions and another activation function for the heady layers. The ReLU function (max(0, z)) does have the disadvantage of losing some information due to treating negative Z values as zero. This problem is solved by the leaky ReLU function: max(c*z, z) where c is an arbitrary constant that uses a proportion of z as the lower limit, e.g 0.001 * z. If z is negative, a small negative proportion of z is the maximum and thus the output of Leaky ReLU.

- Linear or identity activation functions would make adding neural network layers pointless. The composition of a series or sequence of functions is no different from ordinary non-neural-net linear regression. Linear and identity functions are typically only used at the output layer.

- The parameters of each neural network unit or neuron have to be randomized and diverse to avoid identical outputs. Setting the bias to zero or only one value is fine, but the hidden units or neurons should have significantly different initial parameter values.

- The weights should be less than one and are typically very small, e.g. 0.01. This is done to avoid extreme variations and to prevent very small derivatives. Many activation functions have very small derivatives given large negative or positive output. There are certain cases where large weights are good or acceptable.

- There are some neural network functions or formulas that take exponentially more hidden units to compute compared to less hidden units organized into more layers. That is one reason deep neural networks are used in place of shallow networks with many hidden units. This is related to Circuit Theory and the principles of convolutional neural networks.

@@@
Derivatives of TanH, Sigmoid, ReLU, and Leaky ReLU Activations

Sigmoid:
g(z) = 1 / (1+e^-z)
g'(z) = g(z) * (1 - g(z))
g'(z) = a * (1 - a) where a is the output of the activation function g(z) and z is the result of W * X.

TanH:
g(z) = (e^z - e^-z) / (e^z + e^-z)
g'(z) = 1 - (tanh(z))^2
g'(z) = 1 - a^2

ReLU
g(z) = max(0, z)
g'(z) = 0 if z < 0, 1 if z >= 0
*Mathematically, g'(z) if z = 0 is undefined, but it may be treated as zero for neural networks.

Leaky ReLU
g(z) = max(c*z, z)
g'(z) = c if z < 0, 1 if z >= 0

@@@
Working with Numpy Arrays

- When working with Numpy ND-arrays, it is common to place each example or sample as a visual "column" with each row or item in the column holding a feature value. Each "row" holds different samples' values for one feature X. Assume X is an n_rows x m_columns matrix with n rows corresponding to n samples and m columns for m features. Numpy and Pandas present that in the correct visual way by default. When multiplying an (n_example,m_feature) matrix X to a parameter array W with m parameters corresponding to m features, W has to be transposed into a single-column vector for the numpy.dot() function to work. This is consistent with the rules of matrices in mathematics. In mathematics, you take the dot-product by multiplying each row of the first matrix - in this case X - element-wise to the corresponding column of the second.

* In Numpy, a (1, ) matrix is sometimes treated as if it could be both a (, 1) or (1,) matrix. Thus it is recommended by this course to use a (1,n) or (n, 1) matrix, avoiding all matrices with an undefined rank.

- When using Numpy, it is common to use an n_rows by m_column matrix X with each row corresponding to one feature, not one sample. Each column holds multiple features for a single sample. Assume a parameter array W holds n_rows elements. Use W as the first matrix. Take the first and only row of W and multiply each element w to the corresponding column, i.e. feature value. Repeat the process for each column of X. Also, matrix multiplication is not commutative. In this case, Numpy and matrix multiplication rules require that X be transposed if it is the first matrix.

- Multiplying an (n1_row, m1_column) matrix by (n2_row, m2_column) matrix should yield a matrix with (n1_row, m2_column) rows and columns. In matrix multiplication, first process matrix one row one. Take the dot product of matrix one, row one with each column of row two. Place each number corresponding to a dot_product(row1, column_m) result in a new column. Move on to row two. Place the dot product results for row two on the next row of the result matrix. References:
https://www.mathsisfun.com/algebra/matrix-multiplying.html
https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-matrices/a/multiplying-matrices

@@@
Regularization

Logistic Regression
- When applying the calculations to find the derivative of a parameter, the regularization term will be removed from the update amount. The regularization term here reduces the effect of very large negative or positive polynomial/parameter-input product/z values. It penalizes large parameters and input values. For certain activation functions like the TanH and Sigmoid, smaller values are also better than large negative or positive values. Gradient descent is executed faster for some activation functions if the activation values are lower.

- For the logistic regression cost function, this is the regularization term:
λ/2m * Σ(j=1 n) w[j]^2
Where:
n = Number of features.
j = Index for the row/horizontal vector. Index j points to a feature parameter.
m = Number of rows/horizontal vectors.
w = Input or feature value.
* This is taken from notes on logistic regression.
This regularization is also referred to as L2 regularization and, in matrix mathematics, is also equivalent to w.transpose() * w.

- For logistic regression using neural networks, this is the regularization term: 
+ λ/2m * ||w||^2
or, expanding ||w||^2:
+ λ/2m * Σ(i=1 n_layer)Σ(j=1 n_layer-1)(w(i,j l)^2
where:
n_layer denotes w of the current layer.
n_layer-1 denotes a parameter or variable of the previous layer.
j = Number of neurons in the previous layer.
i = Number of neurons in the current layer.
* The matrix norm of a neural network is called the Frobenius norm.
* The L2 norm of a vector is also referred to as its Euclidean Distance.

- The bias (b) may also be regularized, but its effect is negligible in practice if there are many different values for the parameter array w.

- If L1 regularization is used instead, w will be a sparse vector, i.e. it will have many zeroes. Replacing 2m with m makes the regularization value higher and more likely to cancel out some values.
+ λ/m * ||w||^2

Dropout Regularization
- Give each unit or neuron a certain chance to be eliminated. If a neuron is selected for elimination, set its activation output to zero. Later, divide each activation value by the probability of keeping a unit. This keeps the scale of the activation values consistent and keeps the expected value of A constant. When running a prediction or test, do NOT implement dropout. The keep probability can vary per layer. In practice though, zeroing out feature input values is rarely done.

- Dropout regularization forces the network to adjust weights based on a more diverse set of parameters and activation values. Since certain values are randomly zeroed, the network will eventually accumulate adjustments based on certain features while ignoring others. This reduces overfitting/high variance. Dropout regularization shrinks the weights.

- A disadvantage of dropout regularization is that the cost function is not well-defined and difficult to track. Due to the randomness of changes, the changes in J could be random and hard to plot. This can be solved by turning off dropout for debugging or descent checking.
@@@

Early Stopping
- When there is a large divergence between the training and development or cross-validation set, stop the network and start the next iteration. This requires periodically monitoring the performance of the model on the development/cross-validation set at certain intervals. Early stopping might increase development time due to the interactions of the methods used to reduce overfit and to reduce training error.

Normalizing Inputs
- When you normalize, make sure to use the same statistical values for your cross-validation and test set. E.g. use the mean and variance of the training data to normalize the cross-validation/development and test data. It is necessary that your data is a representative sample of production or real-world data.

Derivatives as a Limit
- Estimating the derivative ε based on the ratio of change in x (Δx) from x-ε to x+ε is more accurate. Visually, this is taking the combined height and width of two triangles (one from x to the higher Δx +- ε, another to the lower coordinate pair). It may also be visualized as one large triangle from the lowest to the highest point/coordinate pair. 
References:
https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/single-variable-calculus/limit-definition-of-the-derivative/#top
https://www.berkeleycitycollege.edu/wjeh/files/2012/08/calculus_note_intro_derivative.pdf

Gradient Checking
- Turn the various derivatives of each parameter and bias (W, b) into a vector. Each node parameter and bias has to be included in this reshaped vector. Next, reshape the corresponding parameter and bias values. Calculate the derivative of the parameters and biases using a separate process or code section. Compare the separately calculated array of derivatives to the derivatives of your model.

- Include the regularization term in your calculations. Gradient Checking does not work with dropout regularization. A workaround is to turn dropout off to do gradient checking. Run gradient checking at different intervals of training. The implementation might only be correct during a certain stage or at certain values. Periodic gradient checking is sometimes necessary.

- Compute the difference between the approximated or separately calculated derivatives and the derivatives calculated by your model or algorithm. This is strictly a debugging step since it is computationally expensive or slow.

- If there is a large difference between the approximated and model derivatives, try to identify an outlying derivative from a layer, parameter, or unit.

+++++++++++++++++++++++
Optimization Algorithms

Mini-batch Gradient Descent
- Perform estimation only using a part of your input data. Each epoch will use a subset of the data but will update all parameters as usual. Each epoch will update the parameters using a different batch. One epoch is one pass of forward propagation and backpropagation to adjust parameters and biases.

- Usually, mini-batch sizes run faster if they are an exponent of two.

Exponentially Weighted Averages
- In statistics, this calculation is referred to as the exponentially weighted moving average.
V_t = β*V_t-1 + (1-β)*θt
Beta (β) is a developer-specified parameter between zero and one.

- The larger the parameter, the larger the influence of previous values. A larger β has a similar effect to including the average of more previous examples in the current value V_t. A lower β is equivalent to taking the average of fewer previous examples into account when calculating V_t.

- The multipliers in this equation correspond to an exponentially decaying function. Taking the element-wise product of the input and these weights and then summing them up will also yield the exponentially weighted average. These coefficients will add up to one or very close to one.

- Using the standard V_t equation will result in very low initial values. To correct this bias, correct each V_t using V_t = V_t / (1 - β^t). This has to be done per item so that the term (1 - β^t) is higher in the initial phase and approaches zero the more examples are processed. This is NOT equivalent to V_t = (β*V_t-1 + (1-β)*θt) / (1 - β^t). Bias correction is done every batch or epoch.

RMS prop
- Root mean square propagation uses the exponentially weighted derivatives to update the parameters and biases. To prevent dividing by zero and causing a run-time error, add an extremely small positive value. Here it is denoted as epsilon (ε).
S_dW = β*S_dW + (1 - β)*dW^2
W = W - α*(dW / (sqrt(S_dw) + ε))

- RMS propagation will minimize the effect of larger biases or parameters. The intent is to damp out oscillations of the gradient descent or loss function reduction. RMS prop and EWA reduce the effect of large values. One reason they work is that the initial values of parameters and biases are very far from the correct values. Minimizing their effect in the early stages of estimation reduces random and erroneous changes.

- RMS prop and similar weighted averaging algorithms allow a higher learning rate.

Adam Optimization
- Adam Optimization combines RMS prop with exponentially weighted averages to speed up gradient descent. It updates based on the exponentially averaged derivative with respect to w (V_dw) divided by the square root of the squared sum according to RMS prop (S_dW).
For each mini-batch:
V_dw = β_1 * V_dw + (1 - β_1)*dw
S_dw = β_2 * S_dw + (1 - β_2)*dw^2

Correct the value biases:
V_dw = V_dw/(1 - β_1)
S_dw = S_dw/(1 - β_2)

Update the parameter/matrix W:
W = W - α * (V_dw / (sqrt(S_dw) + ε))
*Use the same equations for db (derivative of the cost function with respect to the bias b)
*For RMS prop, moving averages, and Adam, the recommended β_1 is 0.9. For the squared sums, β_2 is recommended to be 0.99. The ε, according to the authors of Adam, is recommended to be 10^-8. These parameters are rarely tuned, unlike the learning rate.

Learning Rate Decay
- Learning rate decay reduces the learning rate over time. The assumption is that when the estimation is close to the minimum, a smaller learning rate is needed to avoid deviating too much due to data noise or chance.
α = (α / (1 + decayRate * epochNumber))*α_0

- Other formulas for the decay rate may be used. For example, the decay rate can be reduced by some discrete value after a certain number of epochs. Others use a constant k divided by the square root of the epoch count. The result is multiplied to α.

Local Optima
- For high-dimensional data, local optima are very rare. It is more common to have a set of parameters where the derivative is zero and changes to certain sets of parameters may guide the estimation to an acceptable or optimal minimum. It is rare to have a common convergence point where the derivative is zero and it would take excessive shifts that increase error to the levels of random initialization just to find the optimum gradient. A larger number of significant data inputs reduces the likelihood of a local minimum where the error of estimation cannot be reduced by ordinary parameter updates.

- Plateaus are still a problem with high-dimensional data since the derivative may stay at near-zero or zero for a long time despite the parameters being far from optimal. They can make gradient descent slow or give the illusion that it is safe to stop learning.

@@@
Hyper-parameter Tuning
- Sample values at random or using a coarse-to-fine process. Do not use a table or grid. It is hard to make a comprehensive grid of random values without sampling an impractical number of combinations. Sampling coarse-to-fine means sampling at random then gradually limiting your search to the combination groups with the best performance.

- Make sure to sample using a scale that matches the possible hyper-parameter values. For values that may range from near-zero to one, use an exponential or logarithmic scale. Find your minimum and maximum values. Take the base ten logarithm of the start and end points then make a uniformly distributed random sample. The number of samples to make is arbitrary. E.g. for a value that may range from 0.0001 to 1, take the base-10 logarithm of equidistant values. Here, log_10 0.0001 = -4 and log_10 1 = 0. If you need to take five samples, use exponents ranging from -4, -3.2, -2.4...0.2, 1.

- Developers are free to determine the minimum and maximum values of some hyper-parameters such as the number of layers and the beta values of Adam.

@@@
Batch Normalization

- Batch normalization increases robustness versus hyper-parameter changes. Batch norm normalizes the input layer and even the out z or activation a of each layer. After normalizing the layer z or a values, change Z or A using gamma γ and beta β: z = γ*Z_norm + β. Gamma and beta are learnable parameters. Z is usually normalized using Z-score normalization. The normalized Z is often referred to as Z̃.
Z = Z - μ / sqrt(σ^2 + ε)
or
Z = (Z - μ) / σ

- When using batch normalization, you may omit the bias term. This term will have no effect on the result of z. The bias b is a constant added to all W * X products and batch norm uses Z-scores to normalize.

- Setting gamma (γ) and beta (β) to cancel out the mean and standard deviation will practically turn off batch norm. β may also be used as a replacement for the bias. These hyper-parameters have to be updated based on their derivatives similar to parameters W.

- Batch norm reduces random or noisy changes to the estimates by normalizing Z. It also makes weights deeper inside the neural network less affected by the weights in the much earlier layers. If the early layers have large shifts or incorrect weights, the effect of those weights is minimized. Especially during epochs close to initialization, changes have less effect on the weights of deeper layers. Since deeper layers are adjusting with some respect to the standard deviation and mean of previous layers activation values, mere changes in value do not always force a large change to the parameters of deeper layers. 

- One of the side-effects of batch-norm is a form of regularization when done with mini-batches. The mean and variance changes based on the batch used. Since the estimates of the mean and standard deviation are noisy themselves, downstream layers are slightly less likely to fit too much based on certain parameters or neuron activation values. This effect is similar to drop-out regularization, but it is NOT a good replacement for methods explicitly used for regularization.

- During testing, do not compute a new mean and variance for each batch. For the test set, use an exponential moving average that processes and accumulates the mean for all batches. The exponential moving average should be used to calculate z̃. This is because the testing phase usually requires each example to be fed into the model one at a time. As usual, this process will apply to all layers.

@@@
Multi-class Classification

Soft-max
a[L] = (e^Z[L]) / Σ(i = 1, n_layers)t_i
- Here, t_i is a vector containing the probabilities of each class, i.e. t_i = [e^Z|C_1, e^Z|C_2, e^Z|C_3...] where C_n denotes class n.
The output a[L] is a vector since Z is also a vector. Each activation value from the second-last layer will be treated as a value for which a parameter array W must be assigned. The parameters should be set so that e^Z / Σ(i = 1, n_layers)t_i is equivalent to the probabilities that the input data X[m] is a member of each one of the possible classes. 

Soft-max Loss
L(ŷ, y) = -Σ(j = 1, C)(y log_e(ŷ_j))
- Here, the loss function zeroes out all values except the correct value. If the prediction y-hat(ŷ) for the correct value y (1) is equal or close to one, the loss is low. log_e(1) = 0, 1 * 0 = -+0, loss is zero. Next, log_e(0.98) = -0.02020207... The lower ŷ is when y = 1, the larger the loss. log_e(0.1) = -2.3, loss is 2.3. This method is based on the maximum likelihood estimation process in statistics.

- Thus, the cost function for soft-max is:
J(W, b) = (1/m_examples) Σ(i = 1, m)L(ŷ, y)
J(W, b) = (1/m_examples) Σ(i = 1, m)(-Σ(j = 1, C)(y log_e(ŷ_j)))

Back-prop:
- Here, delta is used to denote a partial derivative.
δJ/δz = ŷ - y
or
J'(z(W, b)) = ŷ - y 

+++++++++++++++++++++++++++++++++++++
Structuring Machine Learning Projects

- Single-number evaluation metrics are preferable over multiple metrics. Ideally, an "organic" single-number metric should be used, such as entropy or information gain. If such a metric is not available, multiple metrics may be combined, such as thru the F1 score which combines precision and recall. Be careful with how much weight or ratios you place on the metrics to combine.

- Depending on your project goal, some metrics may be optimized while others only need to satisfy a certain threshold. Those metrics which do not need to be maximuzed or optimized, i.e. metrics that only need to meet a threshold, are called satisficing metrics.

- For extremely large datasets, it is possible to use a much smaller fraction of data for validation and test sets, e.g. 1-2%. There is no hard rule for this. One possible way to set sample size is to use the formula for sample variance deviation.
s(x) = σ / sqrt(n)

Here, the population standard deviation (σ) is the standard deviation of a feature across the entire data set. The training and development sample sizes may be set to stay within a certain standard error. Standard error (s(x)) refers to the ratio or number of standard deviations from the population mean a sample mean will probably be. The difficulty of using this method is the large number of features for some data sets.

- Having a test set that is too small can reduce accuracy. Putting too many samples in the dev/cross-validation and test sets could reduce performance. In contrast, too few validation and test items might not be enough to represent your data set accurately.

- Any data modification that is part of the ML pipeline should NOT be done using the entire data set. Normalization and feature engineering should be done separately for each data split or using metrics taken from the training set. Using summary statistics from the entire dataset to pre-process the data set before splitting into groups or splits risks leaking information or mismatching the data pipeline used during real-world deployment.

- Giving more weight or influence to certain examples might be necessary. For example, certain samples may require very high accuracy and precision due to business rules. Adding a weight to the loss function that increases the loss for such examples is a valid tactic. A custom cost and/or loss function might be necessary for this. In other words, the overall cost function may be J = 1/sum(w) Σ(n=1, m_samples) w * L(y_pred, y_true) where w is a developer-supplied weight that is set higher than one if the example being checked is a priority example.

- If your model does well on the training, validation, and test sets but works poorly in production, the performance metrics and the entire dataset have to be changed. Augmentation and data gathering are good options.

- Surpassing human-level performance on a task is difficult. At that point, human feedback and manual analysis give much less results, even if they are done by domain experts. For these and other reasons, the default proxy for best possible performance is human-level performance. Especially for natural perception tasks, human level is used as a stand-in for the theoretical minimum possible error. The difference between Baye's Optimal Error and your model performance is referred to as avoidable bias.

@@@
Error Analysis

- Check the distribution and characteristics of the samples that are mislabled or have high error. The number of samples to take is arbtirary. Some good references are the normal distribution and sample mean deviation from the population mean. The percentage of mislabled examples from a certain class is also the percentage of the model error you can remove by training for that class.

- Deep-learning algorithms are resistant to random erors, but if the errors have a pattern then the model might apply it. Sadly, evaluating the effect of mislabled data requires some amount of manual checking.

- Error checking may also be used to find which examples are misclassified in the dev/validation and test sets.

- Feature engineering with reference to the error analysis results is also a valid response. If data is missing from the training set or there is a pattern to the underfit, data might need to be augmented.

- Examining the data of correctly predicted examples may corroborate or add to the findings of error analysis. Whether or not to check correct labels depends on the return on resources.

- Make sure that the validation/dev set and test set come from the same distribution.

- Find a balance between designing the system and running another iteration to evaluate performance. Be aware of the gains from design changes, algorithm changes, and possible optimization. Without a proper error test, performance metrics are incomplete. Especially in problems with few references or precedents, evaluating the effect of smaller groups of changes by building and then testing an incomplete system is often better than trying to anticipate and fix problems.

@@@
Mismatched Training and Dev/Test Sets

- The expected or tolerable difference between the training and validation set is skewed for these kinds of data distributions. To help check for overfitting, use a training-dev set. This set should be taken from the training set before feeding into the model. It must have the same distribution as the training set. If the train-dev/train-validation set error is significantly higher than the training error, it is good proof of overfitting.

- If the difference between the error on the train-dev set and validation/test set is high, there is a data mismatch. Consider augmenting your training data and other ways to reduce error. A last resort is to include more data similar to the test and validation sets.

- A large difference between the validation/dev set and test set error is a sign of overfitting to the validation set or information leakage. It might also mean you need more data for the dev and test sets.

- If there is a data mismatch problem, manual error analysis or analysis by a domain expert are viable despite high manual labor cost. Another option is to add weights to problematic classes.

- Simulating or synthesizing data similar to the problematic classes is another good option. When synthesizing or augmenting data, make sure that the augmentations are randomized and do not follow a pattern. Otherwise, the model may recognize the pattern and only correct for that specific error. For example, the noise added to speech in a quiet room should be mostly unique frome example to example. The same applies to image transformations. 

- Make sure the distribution of the data selected for augmentation or synthesis has the same distribution as the train and dev sets. The data used in augmentation should be randomly distributed or at least reflect the real distribution of distortions.
@@@

Transfer Learning
- Transfer learning may be used as a solution for lack of data. If there is neural network with good performance and plenty of training data on a similar problem, that trained network could be modified for your problem. Remove the pre-trained output layer and add a new output layer or more hidden layers. The old neural network weights may be retained throughout the process or trained along with the new layers. Make sure that the pre-trained network has more data than you have for your problem and that the problems are similar, e.g. general image recognition vs. image recognition for radiology images.

- For transfer learning to work, the networks must have the same or compatible input. Transformations to fit data into the first layer are allowed. The reference or pre-trained layer should have had much more data to train on than what is available to you. If possible, check if the low level features of the pre-trained network's problem could be applicable to your problem.

Multi-task Learning
- For some reason, training a neural network to find weights for several very similar tasks often increases performance. The example given was an image recognition problem for self-driving cars. If the output of the model tells whether there are pedestrians, traffic lights, and other objects on screen, the model may perform better overall since it is doing several similar but different tasks. Transfer learning works well for tasks with shared lower-level features.

- If some outputs are missing a label, do not calculate a loss for the unlabled sample.

End-to-end Deep Learning
- For some applications, developing several networks to perform subtasks and then feed their data into a final network is a valid plan. This is especially useful if you do not have enough data for your task. End-to-end learning uses one large and deep neural network to find weights and biases for the final output without using other networks for subtasks. This usually requires a much higher amount of data, but data-gathering has advanced so much that end-to-end learning is common.

- An example of good use for step-by-step learning is in facial recognition. It is common to have separate software to detect and crop or pre-process faces and another to verify identity. By breaking down the problem, the available data is used much more effectively. Identity verification images are usually cropped and close-up. Facial tracking images are from different angles and distances. Since the proper weights for the separate problems are very different, breaking down the problem increases performance. Putting these pipelines together would require an extremely large data-set.

- A counter-example to the split pipeline is with machine translation. There is a sufficiently large sample size of translations than before so a split pipeline is not necessary. Many automatic translators have a single pipeline.

- It is hard to check whether the available data is a match for the complexity of your task. It can be helpful to divide your problem into components and check if they are similar enough or have transferrable data among each other. The specificity or closeness with which data matches your x and y mappings should also be checked. Refer to the example using image classification and self-driving.

+++++++++++++++++++++++++++++
Convolutional Neural Networks

???
Question:
What are the general rules or principles for finding matrix operations that are used as image filters in convolutional neural networks? E.g. per-element multiplication of an image by a three by three matrix with ones in the first column, zeroes in the second, and negative ones in the third.

Convolutions Over Volume
- The channels or depth of the input and the filter have to match. When convolving, multiply the input by the corresponding depth or channel in the filter. Next, take the sum of all the element-wise multiplications across all channels and across the length and width of the filter. That single number - i.e. the sum across all channels, rows, and columns - is one element in the activations or outputs. Shift the filter one column and repeat the process.

- The filters per channel or depth may have different parameters to detect specific colors. The output will still be two-dimensional unless multiple filters are used.

- Multiple filters may be used by convolving them and stacking or appending the result matrices. The result will be (((n + 2p - f)/s) + 1) x (((n + 2p - f)/s) + 1) x c where c is the number of filters. Using a stride of one and padding zero is equivalent to the base fomrula (n - f + 1 ). Here, f is the filter dimension at the corresponding channel.

- If a filter's depth does not match the input, each layer will be applied to all the input values separately. In case the depth of the filter does not match the depth of the input, layers will function as if they are different filters.

- The parameters of the matrix used to "convolve" the image may be set by hand or learned by the model. The convolution operation between the image and filter reduces the dimensions of the data but preserves certain relationships, such as edges or distinct differences in brightness in images. The result of a convolution between an n x n matrix and f x f filter will have the dimensions (n-f+1 ) x (n-f+1).

- Some papers or books define convolution as mirroring the filter along the vertical and horizontal axes before applying it. The convolution described in much of machine-learning literature is technically a cross-correlation, not a convolution according to signal processing or mathematics. A mathematical convolution is associative, i.e. (A * B) * C = A * (B * C).

- The convolution operation may reduce the dimensions of the input. In the cases where it doesn't reduce input dimensions, it still reduces the possibility of over-fitting due to the high number of parameters to learn. This is a side-effect of slight information loss during the convolution.

Padding
- A possible problem with certain filters is that pixels on the edge may have diminished effect on the final result. This could reduce information gained. Next, the input shrinks after every convolution with a smaller matrix. To avoid shrinking and losing information in the corners of the input matrix, the input may be padded with zeroes or a certain number. The amount of padding to maintain size is derived from this formula: n + 2p - f + 1 = n. Here, 2p is used since padding one row and one column to keep symmetry results in two more rows and columns, one for each side.
So,
n + 2p - f + 1 = n
->
p = (f-1)/2

- If f (the dimensions of the filter) is even, you will need asymmetric padding. Still, an odd-valued filter has a centered position. That center is a useful reference for some situations. Using odd filters is the convention in computer vision.

Strided Convolution
- The movement of the filter is increased. Instead of shifting by one row right to left and then one column top to bottom, it shifts two or more rows and then two or more columns. The output dimensions are ((n + 2p - f)/s) + 1 where s is the "stride." If the fraction here is not an integer, round down, i.e. take the floor and bring it down to the nearest integer. The filter has to be fully contained by the input and the padding.

Pooling Layers
- Pooling layers reduce the dimension of the input. They may also make the network more robust against overfitting since they aggregate data, e.g. max-pooling. Reducing the data according to proper limits reduces noise.

- Pooling algorithms have two common hyper-parameters: f and s. These are the filter size (f) and stride (s). In rare cases, the padding of a pooling layer is also used as a hyper-parameter. There are NO learnable parameters in a pooling layer.

Benefits of Convolution
- Even for modern computers, the amount of learnable weights or parameters would be computationally expensive. The parameters used in the filter will be applied to other input groups. In other words, a convolution takes less operations than matrix multiplication. The convolution takes the element-wise products of the filter and a section of input with matching size. A matrix multiplication requires fully matched rows and column element counts.

- Connections to convulution layers are sparse connections. One activation output is only affected or connected to a subset of the previous layer's activations.

Case Studies
- It is common to place pooling layers between one or more convolution layers. Some approaches had many hyper-parameters for the filters, like AlexNet and LeNet. Others have learnable parameters instead, but may be resource hungry and deep, like VGG-16.

- Vanishing and exploding gradients are problems affecting deep networks. The gradients may naturally get very small or large due to the number of layers. A residual block in a residual neural network fast-forwards data in some sections. The activations from one layer are added to the activations of a deeper layer. The activation for that target deep layer would be g(z[l] + a[l-n]).

Residual Neural Network/ResNet
- In cases where the linear computation part of the activation is zero or near-zero, a residual block would replace those values with the activation result of the layer where the block started, a[l]. The network will be able to learn parameters that adjust to the element-wise addition.

- In case the z and a[l] have different dimensions, add a set of parameters that pad or reshape a[l] to match z. They may be manually set or learnable parameters.

- A one-by-one convolution outputs a number by convolving one number across the depth or channels of the input. This will preserve the n and m dimensions of the input while having a depth equal to the number of filters applied. This convolution is also called a network-in-network convolution. This convolution may be used to reduce the depth or channels of the input data.

- Using a one-by-one convolution to reduce the depth or channels of input before applying a larger convolution will significantly reduce the operations needed

Inception Network
- The Inception network appends the result of serveral filters and poolings depthwise. To reduce the layers needed for each filter, a one by one convolution is used before applying the filter or pooling. In some layers, a prediction is made using the activation function. The result is added to the stack. This has a regularizing effect.

Mobile Net
- Mobile Net arranges the convolutions in a way that reduces the number of operations. V1 uses a single filter to downsize the input. Next, it will apply one or more one by one filters on the reduced input. It manages to get similar performance for less computations.

- Mobile Net V2 uses several one by one filters to increase the depth of the input before applying a single filter to downsize it. After downsizing, one or more one by one filters will be used. Mobile Net V2 also applies residuals. It takes the input before the first filter and adds it to the last output of the "bottleneck" block.

@@@
Detection Algorithms

Sliding Windows
- Converting fully-connected layers into convolutional layers removes the need to re-compute predictions for several windows. Instead, the filters and strides will be used to traverse the input. In the convolutional implementation, one forward propagation step will be used for all windows in the image. Fully connected layers would have been run once for every window. Since they are now convolutions, the windows will be calculated and returned as one matrix. This way, the forward and backward propagation steps will be done for all windows all at once instead of sequentially, one cycle per window.

Intersection over Union
- The IOU is the conventional way to measure overlap between image boundaries. It takes the ratio of intersection (pixels or values present in both boundaries or sets) and divides by the union (unique elements present in either or both of the sets). This method may be used to measure bounding box accuracy. It may be used to remove image detections that overlap and are redundant with an arbitrary reference set/image boundary.

Non-max suppression
- If there are several boundaries or anchors that have high overlap, pick the window with the highest probability of containing any object as a center point. Remove windows or bounding boxes with a lower probability and high overlap with the center point.

- Bounding boxes that do not meet a certain probability of holding an object should be removed before non-max suppression. A possible defect of this method is that two different objects with overlapping boundaries and high overlap might be removed. To prevent this, perform non-max suppression separately for different object classes.

Anchor Boxes
- Anchor boxes allow the model to make multiple boundary boxes for different objects with the same midpoint. If a person and a car have the same midpoint, the boundary could be too tall or too long without an anchor box. An anchor box increases the elements of the activation vector and gives separate outputs for each anchor box. The anchor box is a pre-defined shape typical to a certain class. This will allow the model to make different boundaries for a human and a car that share the same midpoint.

- The anchor box shapes may be set manually or based on an algorithm, e.g. unsupervised K-means clustering. To pick which anchor box is best suited to a certain object, use a metric like IOU.

Transpose Convolutions
- There are many methods to expand or blow up input. This is one method. A filter will be placed on top of the output or, at least, with reference to the output. Take the element-wise product of the filter and the element in the first row and first column of the input. The resulting matrix should be placed with the matching corner in the first row and column of the output. If padding is used, the filter output should be placed in the padding. Output elements within the padding bounds are removed.

-  After the filter output is placed in the output matrix, use the next element in the input. At the same time, shift the filter location on the output matrix according to the stride. Broadcast the current input element across the filter as before. In case of overlap between different filter outputs on the output matrix, add the overlapping values.

- Be sure that the parameters and dimensions of the input and filter do not force the filter output out of bounds. The movement of the filter in the output should correspond to the movement and location of the focus in the input.

U-Net Architecture
- This transpose convolution is necessary for the U-Net architecture. The transpose convolution is necessary to match the dimensions of the output with the dimensions of the input. A U-Net is used for semantic segmentation of images, so each pixel or image region has to be classified individually.

- The transposition of input in the U-Net architecture is combined with several residual blocks. This combines the previous input - i.e. the image or convolutions of the image - with the block output. The transposition of the convolution layers will retain some patterns that emerge from the convolution, e.g. edges or shapes. Adding the original image or convolutions to the output will overlay the transpoistion of the patterns with the original image or convolutions. The goal is to make the model learn parameters to classify pixels based on the original convolutions, the original image, and the patterns detected during feed-forward convolutions.

Triplet Loss Function
- The triplet loss function requires three images or inputs: an anchor, a positive (match) example, and a negative example (non-match). Loss:
L = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + α, 0)

- Here, alpha (α) is a parameter for the required margin or difference. The function f encodes the image. The differences between the encoding of the positive example f(P) less the encoding of the anchor image f(A) should be as small as possible. The difference between the encoding of the anchor image f(A) and negative example f(N) should be as high as possible. If the difference is the same, the margin forces the loss to go above zero to stop the algorithm from treating all images as fifty-fifty matches.

- It is required to choose triplets that are hard to solve. Similar images of different people are necessary to train the function in recognizing similar but different faces. For deployed facial recognition systems, the encoding of positive examples may be pre-computed. The model would only need to find a match.

+++++++++++++++
Sequence Models

@@@
Recurrent Neural Networks

- To allow earlier words to influence the activation of later words, the activation array from the previous word will be included in the calculations for the next word. A dummy initiazliation value will be included when necessary. After processing the first word of the input and making a prediction on it, the model will take the activations and use them in calculating the activations of the next word. Each layer will have parameters for the previous activations and parameters for the current input word. In the illustrations and videos of A. Ng's Sequence Models - Recurrent Neural Network Model, the arrows from layer to layer indicate a change in time or input. The layer in question is the same thruought the video and slides.

- The basic RNN described here stacks the results of Wax and Waa or Wa * [a, x]. The addition operation does not mean adding the matrices element to element.

- Sequence generators and language models include the preceeding input in the information passed to the model in the next pass. After processing the first part of an input sequence and generating a prediction, the model is given all the preceeding values x[0, n-1] before processing the current x[n]. Generally, sequence generation requires a large dataset. Language models require a large corpus or body of text. Sequence models may be used to predict the chance of a certain tokenized element (e.g. a word) given a certain preceeding input set. Such a prediction requires that the tokenized elements are represented in a dictionary or vocabulary. Notes, words, unknown words, and control characters have to be represented. One way is one-hot encoding.

- Word-level language models are the norm, but character-based language models are an option. Character-level models use a vocabulary with characters instead of wrods. They avoid unknown characters or items missing from the vocabulary. They are resource-intensive though, and usually don't capture relationships that are separated by many words.

- Exploding and vanishing gradients are possible defects for RNNs, especially vanishing gradients. Exploding gradients often result in NaN paramters or overflow errors due to large values. Gradient clipping and other regularization methods usually avoid this problem, but another solution is required for vanishing gradients. Vanishing gradients prevent the errors computed for earlier words from having a strong effect on the backpropagation step.

Gated Recurrent Unit
- A memory cell c will be assigned the activation value of the previous input (c<t-1>, a.k.a a<t-1>). A new activation, c̃ (c-tilde), will be computed. The c̃ takes the previous activation c<t-1>/a<t-1> and input x as parameters. A gate parameter Γ (gamma) will be calculated using learnable bias and parameters separate from c̃. The gate parameter is a value between zero and one that will be used to combine the values of c and c̃. Zero or one will set the activation a<t> to either c (a<t-1>) or c̃.

- There are cases when a previous input (e.g. the subject of a spoken or written sentence) influences a distant section of the input. The encoded value of the previous input may be unable to influence related but distant inputs. For example, take the sentence "The cat, which ate fruits..., was full." The encoding of "cat" may not be able to shift the gradient and teach the model that "was" is singular because of "cat." The middle phrase may dilute the influence of "cat/cats" on "was/were".

- Remember that c<t-1> is calculated not just using the last input element but all previous input elements.

Long Short Term Memory (LSTM)
- An LSTM uses an update gate like the GRU. An LSTM has a separate "forget" gate with a separate parameter Γ_f that controls how much of the previous activation value c<t-1> is retained. This is in contrast to GRUs which simply use the term (1-Γ_u). A Long Short Term Memory model may add the full values of c̃ and c<t-1>. The LSTM has a third gate called the output gate. These gates each have their own learnable parameters and biases. The output gate determines what proportion of c (after processing it with c̃) is returned as the activation value a. GRUs simply return c<t>, so in a GRU, a<t> = c<t>. LSTMs pass c<t> thru a tanh or other activation, tanh(c<t>).

- An LSTM model may have a relevance gate Γ_r (capital gamma) like GRUs, but the common version doesn't. Recall that GRUs compute Γ_r (relevance) using a - usually sigmoid - activation function that takes c<t-1> as input. GRUs use Γ_r in calculating c̃. The common LSTM doesn't, but this peephole version does. However, the peephole LSTM retains the separate forget gate to control the merging of c and c̃.

- When computing the output gates of an LSTM, the values of c<t-1> may be combined with the values of x<t>. This is called a "peephole connection." Here, c<t-1> is the "memory cell" output computed based on a<t-1>.

- There is no consensus on which is universally better between LSTMs and GRUs.

Bi-directional Recurrent Neural Network (BRNN)
- A bi-directional neural network processes input in two passes. The first pass processes input from left to right. The second pass processes input right to left and adds the new values to the forward activation functions. Corresponding forward and backward predictions will be aggregated. Next, backpropagation will be done as normal. The graph of a BRNN is an acyclic graph.

- When RNNs are stacked or multi-layered, the first input will be pushed thru all the layers until a prediction is made. In a BRNN, the backward pass will start after the entire input set (sentence, phrase, or paragraph) is fed forward. It is possible to feed the output of the RNN to a normal neural network.

@@@
Word Embeddings
- Words may be represented using matrices of features. Usually, the features learned by a model that generates word embeddngs or encodings are hard to interpret. Still, these encodings can help describe the differences and similarities between words. Take word analogies as an example. The vectors between the pairs man-woman and king-queen will be similar. Those pairs would have different values but similar changes in value along different axes. This is because the arrangement, frequency, chance of occurance, and other quantifiable features can be learned by an encoder model. Since both word pairs are used in a similar way, their features or embeddings will also have some similarities. For example, they should have a similar chance of occuring after or in relation to pronouns like he, she, him, and her.

- Word embeddings may be learned. A matrix may be used to represent vectors of word features. Each column or row is a vector of feature values. The rows or columns correspond to a fixed word dictionary. Assuming the words are represented using a one-hot vector corresponding to the word dtictionary, a one-hot encoding of a word O could be multiplied to a word embedding matrix E. The resulting sparse matrix would have the values of E corresponding to feature values of O. The values of E can be represented as learnable variables.

- Visualizing word embeddings may show clusters or patterns among the features of certain words. However, word embeddings are not guaranteed to be human-interpretable. Despite the patterns present in word embeddings, the reason for such patterns may not always be inferred after all the calculations done to get the encodings/embeddings.

- Large pre-trained language models can be modified as part of transfer learning to work using much smaller data sets. Networks trained on a large corpus may be transferred to a different but similar task. The embeddings and some earlier layers may also need to be canged in addition to later layers.

- Analogies between words may be learned using similarity measures as cost functions, e.g. cosine similarity and the negative euclidean distance. A probabilistic model may be used to learn word sequences or the likelihood of a certain word given some input. The "context" - range of words used as reference - used as training input is a hyperparameter.

Word2Vec
- Word2Vec attempts to learn a matrix of embeddings E using a softmax final layer vs. the entire dictionary. It will adjust the feature vector (e) of a specific word to minimize the error when predicting P(t|c). P(t|c) is the probability of encountering a target word t within a context/range of words c. The disadvantage of this method is the slowness of softmax and its alternatives (e.g. heirarchical softmax, based on binary search).

Negative Sampling
- Negative sampling functions similarly to Word2Vec but reduces the number of alternative/negative probabilities in the softmax. It will take a certain positive example where the context is known to contain a target word. At random, other contexts will be selected at random. These contexts may be a single word in the dictionary. Given one positive example of context and several negative examples, the learning problem is now P(t|c,t) = sigmoid(E.transpose() * e<context>). For every word in the dictionary, Word2Vec computes a softmax vs. every other word in the dictionary. Negative sampling trains one positive example and an arbitrary amount of negative examples.

- Negative sampling requires proper sampling of the negative examples. There has to be a balance between teaching the model using frequent words and some less-common and obscure words. The distribution must reflect actual use of the language's words.

GloVe Vectors
- GloVe (Global Vectors for word embeddings) optimizes by the summation:
Σ_i,n_dictionary Σ_j,n_dictionary f(X_i,j) (O.transpose() * e_j + b_i + b_j - logX_i,j)

Here,
f(X_i,j) = Regularization term to set log 0 to zero instead of undefined.
X_i,j = The number of times j appears within proximity of i. The proximity limit is called the "context of i."

- GloVe takes the summation of the number of times each word appears within the context of every other word in the dictionary. It will adjust parameters of E so that E.tranpose() * e_j corresponds well to whether or not certain words appear close to each other.

Sentence Level and Higher-level Embeddings
- Embedding a sentence or paragraph has the advantage of being able to calculate the context of all the words. In word-by-word embeddings, sentences with similar words but different meaning due to arrangement are better reflected by sentence level embeddings. "The kids play in the park" and "The play in the park was for kids" have a very different meaning. Word embeddings usually mark those sentences as being more similar than do sentence embeddings.

@@@

Beam Search
- Beam Search is an alternative to a greedy search of likely words. A greedy search is likely to miss context that changes the probability of certain word mappings. If a certain word usually follows another, it may become the de-facto "best translation" even for certain sentences where the most frequent and most likely word isn't the best translation.

- Beam search has a hyperparameter that determines how many of the most likely translations will be "remembered" and evaluated. For each of the words or encodings included in the "beam," beam search will pick a beam-sized set of likely next tokens. Next, a beam-sized set of candidate next values will be picked for each of the pairs. The probability of each two-step-ahead pair will be evaluated. A beam-sized set of two-element sequences will be selected. At every step of the beam search, an instance of the network will be instantiated for each token in the beam.

- To avoid numerical underflow, a beam search algorithm is usually taught to maximize the sum of logarithms of P(y<t>|x,y<1>,...y<t-1>). Beam search maximizes the product of probabilities Π (capital pi): Π_t=1,Ty P(y<t>|x,y<1>,...y<t-1>). Practically, take the sum Σ (capital sigma) of logarithms instead to avoid values that cannot be represented. Σ_y=1,Ty log P(y<t>|x,y<1>,...y<t-1>). The log (a.k.a appropriate exponent) of a non-zero base is higher the higher the target value or "argument."

- A side-effect of maximizing the summation of logarithms is that longer sequences will get a lower probability because multiplying numbers less than one decreases overall value. The summation has to be normalized. One way is to take the average of the logarithms (e.g. multiply by 1 / Ty). A hyperparameter α ranging from zero to one may be used as an exponent to Ty. There is no theoretical proof this works, but most experiments show 1/Ty^α is a useful normalization term.

- The cumulative probability of each possible sequence will be compted according to the probability summation function. For the current element in the sequence, a beam-sized set of candidate next steps will be chosen. For each of the candidates, more beam-sized sets of "grandchildren" will be selected. The probabilities of all these branches will be calculated. Based on that, A beam-sized set of the top sequences will be chosen for the current step. The cycle then repeats.

- To check whether the beam search stage or RNN stage is at fault, check the cumulative probability of the ground truth label y* and the output ŷ. If the probability of y* is higher, the RNN is probably computing the probability correctly but beam search is not maximizing the P(ŷ|x,y_n...). If P(y* |x) >= P(ŷ|x), the RNN needs to be more accurate. Check how many errors are due to a bad output from the RNN and how many are due to a failure to find the best ŷ during beam search.

- If the loss or cost function includes length normalization, evaluate P(y*) and P(ŷ) using a function that takes the normalization term into account. Special considerations have to be made when evaluating a beam search that uses length normalization.

Bleu Score
- This measures accuracy by checking how many times certain words and combinations of words in a model output actually appear in the ground truth label or labels. The number of times a certain word or word combination may be counted is limited to the number of times such a set appears in the labels. For example, "the cat" may only be counted once for a label "The cat is fat." This avoids giving a high score to a translation like "The cat is the cat." The ratio of clip counts (number of times the set appears in the label) to the counts (number of times the set appears in the output) is taken per set. Several sets of words may be used to get a Bleu Score. The score will be averaged.  There is also a brevity penalty that reduces the score of a translation the shorter it is compared to the label. This avoids the side-effect of short sentences having higher computed probabilities due to how numbers work.

Minimum Bayes Risk
- Minimum Bayes Risk is another accuracy metric. 
arg_max 1/n * Σ(Metric(E, E`))
here:
Metric may be a loss function or measure of similarity. Multiple translations have to be generated. For each candidate, get the score or metric relative to every other translation. This is a one vs. all comparison. Do the comparison for all translations and pick the one with the highest average similarity score. This assumes that all the translations taken together tend toward the correct translation.

Attention Model
- An attention model encourages a model to give greater weight to tokens that are at and near the current input. An RNN or BRNN will be used to compute features or encodings of the input sequence. After the entire sequence is processed, the resulting sequence of encodings will be processed by the attention model. The elements are processed sequentially. An attention parameter α (alpha) will be used to calculate the context for all tokens in the sequence. For each input element, a "context" is calculated using α and other tokens/encodings in the sequence. The context is used to generate an activation to predict a translation. When the next input element, i.e. encoding, is processed, the previous output is used along with a new context to generate a succeeding prediction. The attention parameter α has to be the same for the entire sequence.

Connectionist Temporal Classification (CTC) Cost Function for Speech Recognition
- The CTC is used for audio processing problems. The output of audio may be very long since the audio is sampled at a very high rate, e.g. several times a second. The output may have blanks where there is no meaningful audio. It will also have long repeated sequences of letters for stretches of audio lasting a few milliseconds that make up one letter. CTC collapses these repeated letters and blanks into human-readable transcripts. It may also be used in trigger-word detection. In trigger-word detection, the entire audio sequence that holds the trigger word is labelled as one or positive to counter the large number of non-trigger audio sequences.

@@@
Transformers

Self-attention
- Calculate the attention matrix of the current token:
A(q, K, V) = Σ_i ((exp(q*k<i>) / Σ_j exp(q*k<j>))*v<i>)
Here:
q (query), K (key), and V (value) are matrices computed using learnable weights. Each word or token in the sequence will be assigned a different set of W * q, W * k, and W * v matrices. The matrix A of the current token A<t> will be calculated relative to its own set of parameters and vectors - W * q, W * k, and W * v - and the matrices K and V, which are the computed W*k and W*v vectors of other tokens in the sequence. Here, q, k, and v are representations/tokenized encodings of each word. When embedding, it is the convention to have q = k = v, i.e. the tokenized representation of a sequence is the same for the W_q, W_k, and W_v matrices. These encodings multiplied to W_q, Q_k, and W_v will result in Q, K, and V matrices for each word.

- A<t> will be computed as the soft-max of A(q, K, V)<t> vs all other tokens in the sequence, Σ_j A(q, K, V)<0,..j>. A of the current token may also be represented as A(q<t>, K, V). Take note that matrix multiplications are NOT commutative. The result of matrices A * B will be different from B * A. The resulting A<t> here will be a matrix of ratios. The softmax and A(q, K, V) computations will guarantee that information from all words in the sequence will influence A<t>, the matrix representation of the current token.

- A_1,...An will be computed in parallel for all tokens in the input of a transformer network. One advantage of this process is that A is not a fixed word embedding. The representation/encoding A of a token will not be based on a large corpus. Based on the sequence currently being processed, the model will generate a representation for the token.

- There is an alternative notation in literature for calculating the self-attention equation/matrix A of the current token. In the equation below, the denominator term is used to scale the dot product to prevent an explosion.
A(Q, K, V) = softmax(Q*K.transpose() / sqrt(d_k))V

Multi-head Attention
- In Multihead(Q, K, V), several instances of A(Q, K, V) will be run simultaneously for the current token vs. all other tokens in the sequence. The number of heads is a hyper-parameter. The concatenation of the results from the heads will be used to compute the output of the multi-head attention equation. The parameter matrices W are different for each head. Each head has different parameters W for each Q, K, and V. The goal is to have different versions of the Q, K, and V matrices per head.

Multihead(Q, K, V) = concat(A<head_1,...head_n>) * W_o
where:
A<head_n> = Attention(W_i*Q, W_i*K, W_i*V)

- The full transformer network has separate encoding and decoding stages. The input sequence is turned into a basic vector encoding and then passed to the encoder stage. The encoding should include the position of the token in the sequence. In the encoding stage, the Q, K, and V values of all the tokens will be computed. First, a multi-head attention calculation will be performed for each of the input tokens. The results will be passed to a feed-forward neural network. Encoder layers of this structure will be stacked or repeated N times. The paper recommends N=6 stacked encoder layers. After the encoding stage runs N times, the results will be passed to the decoder. In the decoder, the start-of-sentence token will be fed in at the beginning. The Q matrix will be calculated by the first multi-head block of the decoder based on the start-of-sentence (<SOS>) token. In the next block, the Q matrix of the first decoder block will be processed with the K and V matrices from the encoder in another multi-head attention layer. The decoder will feed the result to a feed-forward neural network, usually with a ReLU activation. Decoder layers will also be stacked/repeated N times. Finally, the decoder will output the next token after <SOS> using a linear layer with a softmax activation to predict a translation. This output translation will be passed back into the beginning of the decoder block so that the next token can be predicted. The decoding cycle will repeat until the end-of-sentence <EOS> token is reached.

- In the decoding stage, the true or correct next token is hidden from the decoder to prevent information leakage. While a certain token is being processed, the correct tokens after it are hidden. The correct previous tokens may also be provided during training. Hiding the correct future tokens is referred to in the paper as "masked multi-headed attention."

- The position of a word is also added to enrich context. The vector encoding the token position will be added directly to the input vector before starting the encoding loop.
PE(pos, 2i) = sin(pos / 10000 * (2i/d))
and
PE(pos, 2i + 1) = cos(pos / 10000 * (2i/d))
in both:
pos = Numeric position of the current token in the sequence.
i = Current position being calculated for the PE vector, 0 - PE.length, depends on network shape.
These will be represented as vectors.

- After every multi-head and neural network block, residuals will be added and normalization will be run in an "Add[ition] & Norm[alization]" layer. This has an effect similar to batch normalizaion. It makes gradient descent smoother.

@@@
Transformer Network Encoder and Decoder Architecture According to "Attention is All You Need"
- Here, the encoder maps an input sequence of symbol representations (x1 , ..., xn ) to a sequence
of continuous representations z = (z1 , ..., zn ). Given z, the decoder then generates an output
sequence (y1 , ..., ym ) of symbols one element at a time. At each step the model is auto-regressive
[9], consuming the previously generated symbols as additional input when generating the next.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder

Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [10] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512.
*Note: Here, 512 is the dimension of the demonstration model. It is not a standard.

Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position i can depend only on the known outputs at positions less than i.
@@@

Teacher Forcing
- In "teacher forcing", the true label of a previous token is used as the input to the decoder regardless of whether it got the last token right. This is to avoid a cascade of errors. This forcing may be gradually reduced as training continues.

Transformers vs. RNNs and BRNNs
- RNNs, despite the implementation of attention, are may still lose information the further tokens are from each other. Sequential models process tokens sequentially, so there are less ways to parallelize their implementation. With RNNs and especially deep deep RNNs, there are more steps taken during back-propagation than transformers. RNNs perform back-propagation for each input token. Transformers take around one step from output to gradients during a cycle of back-propagation. Last, transformers are less likely to cause vanishing gradients even vs. LSTM and GRU transformer nets.



