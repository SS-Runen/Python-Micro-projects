++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using sample input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

- Numpy arrays and Pandas data frames list row count and "column" count in their shape like so: (n_rows, n_cols). Technically these are just nested arrays with two dimensions. Flattening them would show n_rows as elements. Each element holds n_cols sub-elements.

- To test if values meet conditions in Python, use the assert() method. E.g. assert(Z.shape[0] == w.shape) or assert Z.shape[1] == W.T.shape[1], "Z column count mismatch with W.transpose column count."

+++++++++++++++++++++
Neural Network Basics

References for Derivative Rules:
https://www.mathsisfun.com/calculus/derivatives-rules.html
https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/economics/differentiation-and-integration/rules-of-differentiation.html
https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf

Reference with some proof examples:
https://www.sfu.ca/math-coursenotes/Math%20157%20Course%20Notes/sec_DerivativeRules.html

- The derivative of the cumulative cost function J(vector(w), vector(x)) is the average loss for the training set. Because of that, the derivative of the cumulative cost function with respect to a parameter w is the average of derivatives for the per-item loss function L(p̂ - p) where p̂ is the prediction and p is the correct value or label.

- The Sigmoid and TanH (Hyperbolic Tangent) functions have a slope that is close to zero whenever Z (the resulting value of the parameter and input value polynomial) is very large or very small. This means gradient descent proceeds slowly for very large or very small values. The TanH function is a shifted version of the Sigmoid function that allows values between -1 and 1. The ReLU function performs just as well or better than Sigmoid or TanH functions in hidden and non-output layers, but is significantly faster to calculate. It is common to have the final output layer use the Sigmoid function for binary or multi-class functions and another activation function for the heady layers. The ReLU function (max(0, z)) does have the disadvantage of losing some information due to treating negative Z values as zero. This problem is solved by the leaky ReLU function: max(c*z, z) where c is an arbitrary constant that uses a proportion of z as the lower limit, e.g 0.001 * z. If z is negative, a small negative proportion of z is the maximum and thus the output of Leaky ReLU.

- Linear or identity activation functions would make adding neural network layers pointless. The composition of a series or sequence of functions is no different from ordinary non-neural-net linear regression. Linear and identity functions are typically only used at the output layer.

- The parameters of each neural network unit or neuron have to be randomized and diverse to avoid identical outputs. Setting the bias to zero or only one value is fine, but the hidden units or neurons should have significantly different initial parameter values.

- The weights should be less than one and are typically very small, e.g. 0.01. This is done to avoid extreme variations and to prevent very small derivatives. Many activation functions have very small derivatives given large negative or positive output. There are certain cases where large weights are good or acceptable.

- There are some neural network functions or formulas that take exponentially more hidden units to compute compared to less hidden units organized into more layers. That is one reason deep neural networks are used in place of shallow networks with many hidden units. This is related to Circuit Theory and the principles of convolutional neural networks.

@@@
Derivatives of TanH, Sigmoid, ReLU, and Leaky ReLU Activations

Sigmoid:
g(z) = 1 / (1+e^-z)
g'(z) = g(z) * (1 - g(z))
g'(z) = a * (1 - a) where a is the output of the activation function g(z) and z is the result of W * X.

TanH:
g(z) = (e^z - e^-z) / (e^z + e^-z)
g'(z) = 1 - (tanh(z))^2
g'(z) = 1 - a^2

ReLU
g(z) = max(0, z)
g'(z) = 0 if z < 0, 1 if z >= 0
*Mathematically, g'(z) if z = 0 is undefined, but it may be treated as zero for neural networks.

Leaky ReLU
g(z) = max(c*z, z)
g'(z) = c if z < 0, 1 if z >= 0

@@@
Working with Numpy Arrays

- When working with Numpy ND-arrays, it is common to place each example or sample as a visual "column" with each row or item in the column holding a feature value. Each "row" holds different samples' values for one feature X. Assume X is an n_rows x m_columns matrix with n rows corresponding to n samples and m columns for m features. Numpy and Pandas present that in the correct visual way by default. When multiplying an (n_example,m_feature) matrix X to a parameter array W with m parameters corresponding to m features, W has to be transposed into a single-column vector for the numpy.dot() function to work. This is consistent with the rules of matrices in mathematics. In mathematics, you take the dot-product by multiplying each row of the first matrix - in this case X - element-wise to the corresponding column of the second.

* In Numpy, a (1, ) matrix is sometimes treated as if it could be both a (, 1) or (1,) matrix. Thus it is recommended by this course to use a (1,n) or (n, 1) matrix, avoiding all matrices with an undefined rank.

- When using Numpy, it is common to use an n_rows by m_column matrix X with each row corresponding to one feature, not one sample. Each column holds multiple features for a single sample. Assume a parameter array W holds n_rows elements. Use W as the first matrix. Take the first and only row of W and multiply each element w to the corresponding column, i.e. feature value. Repeat the process for each column of X. Also, matrix multiplication is not commutative. In this case, Numpy and matrix multiplication rules require that X be transposed if it is the first matrix.

- Multiplying an (n1_row, m1_column) matrix by (n2_row, m2_column) matrix should yield a matrix with (n1_row, m2_column) rows and columns. In matrix multiplication, first process matrix one row one. Take the dot product of matrix one, row one with each column of row two. Place each number corresponding to a dot_product(row1, column_m) result in a new column. Move on to row two. Place the dot product results for row two on the next row of the result matrix. References:
https://www.mathsisfun.com/algebra/matrix-multiplying.html
https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:multiplying-matrices-by-matrices/a/multiplying-matrices

@@@
Regularization

Logistic Regression
- When applying the calculations to find the derivative of a parameter, the regularization term will be removed from the update amount. The regularization term here reduces the effect of very large negative or positive polynomial/parameter-input product/z values. It penalizes large parameters and input values. For certain activation functions like the TanH and Sigmoid, smaller values are also better than large negative or positive values. Gradient descent is executed faster for some activation functions if the activation values are lower.

- For the logistic regression cost function, this is the regularization term:
λ/2m * Σ(j=1 n) w[j]^2
Where:
n = Number of features.
j = Index for the row/horizontal vector. Index j points to a feature parameter.
m = Number of rows/horizontal vectors.
w = Input or feature value.
* This is taken from notes on logistic regression.
This regularization is also referred to as L2 regularization and, in matrix mathematics, is also equivalent to w.transpose() * w.

- For logistic regression using neural networks, this is the regularization term: 
+ λ/2m * ||w||^2
or, expanding ||w||^2:
+ λ/2m * Σ(i=1 n_layer)Σ(j=1 n_layer-1)(w(i,j l)^2
where:
n_layer denotes w of the current layer.
n_layer-1 denotes a parameter or variable of the previous layer.
j = Number of neurons in the previous layer.
i = Number of neurons in the current layer.
* The matrix norm of a neural network is called the Frobenius norm.
* The L2 norm of a vector is also referred to as its Euclidean Distance.

- The bias (b) may also be regularized, but its effect is negligible in practice if there are many different values for the parameter array w.

- If L1 regularization is used instead, w will be a sparse vector, i.e. it will have many zeroes. Replacing 2m with m makes the regularization value higher and more likely to cancel out some values.
+ λ/m * ||w||^2

Dropout Regularization
- Give each unit or neuron a certain chance to be eliminated. If a neuron is selected for elimination, set its activation output to zero. Later, divide each activation value by the probability of keeping a unit. This keeps the scale of the activation values consistent and keeps the expected value of A constant. When running a prediction or test, do NOT implement dropout. The keep probability can vary per layer. In practice though, zeroing out feature input values is rarely done.

- Dropout regularization forces the network to adjust weights based on a more diverse set of parameters and activation values. Since certain values are randomly zeroed, the network will eventually accumulate adjustments based on certain features while ignoring others. This reduces overfitting/high variance. Dropout regularization shrinks the weights.

- A disadvantage of dropout regularization is that the cost function is not well-defined and difficult to track. Due to the randomness of changes, the changes in J could be random and hard to plot. This can be solved by turning off dropout for debugging or descent checking.
@@@

Early Stopping
- When there is a large divergence between the training and development or cross-validation set, stop the network and start the next iteration. This requires periodically monitoring the performance of the model on the development/cross-validation set at certain intervals. Early stopping might increase development time due to the interactions of the methods used to reduce overfit and to reduce training error.

Normalizing Inputs
- When you normalize, make sure to use the same statistical values for your cross-validation and test set. E.g. use the mean and variance of the training data to normalize the cross-validation/development and test data. It is necessary that your data is a representative sample of production or real-world data.

Derivatives as a Limit
- Estimating the derivative ε based on the ratio of change in x (Δx) from x-ε to x+ε is more accurate. Visually, this is taking the combined height and width of two triangles (one from x to the higher Δx +- ε, another to the lower coordinate pair). It may also be visualized as one large triangle from the lowest to the highest point/coordinate pair. 
References:
https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/single-variable-calculus/limit-definition-of-the-derivative/#top
https://www.berkeleycitycollege.edu/wjeh/files/2012/08/calculus_note_intro_derivative.pdf

Gradient Checking
- Turn the various derivatives of each parameter and bias (W, b) into a vector. Each node parameter and bias has to be included in this reshaped vector. Next, reshape the corresponding parameter and bias values. Calculate the derivative of the parameters and biases using a separate process or code section. Compare the separately calculated array of derivatives to the derivatives of your model.

- Include the regularization term in your calculations. Gradient Checking does not work with dropout regularization. A workaround is to turn dropout off to do gradient checking. Run gradient checking at different intervals of training. The implementation might only be correct during a certain stage or at certain values. Periodic gradient checking is sometimes necessary.

- Compute the difference between the approximated or separately calculated derivatives and the derivatives calculated by your model or algorithm. This is strictly a debugging step since it is computationally expensive or slow.

- If there is a large difference between the approximated and model derivatives, try to identify an outlying derivative from a layer, parameter, or unit.

+++++++++++++++++++++++
Optimization Algorithms

Mini-batch Gradient Descent
- Perform estimation only using a part of your input data. Each epoch will use a subset of the data but will update all parameters as usual. Each epoch will update the parameters using a different batch. One epoch is one pass of forward propagation and backpropagation to adjust parameters and biases.

- Usually, mini-batch sizes run faster if they are an exponent of two.

Exponentially Weighted Averages
- In statistics, this calculation is referred to as the exponentially weighted moving average.
V_t = β*V_t-1 + (1-β)*θt
Beta (β) is a developer-specified parameter between zero and one.

- The larger the parameter, the larger the influence of previous values. A larger β has a similar effect to including the average of more previous examples in the current value V_t. A lower β is equivalent to taking the average of fewer previous examples into account when calculating V_t.

- The multipliers in this equation correspond to an exponentially decaying function. Taking the element-wise product of the input and these weights and then summing them up will also yield the exponentially weighted average. These coefficients will add up to one or very close to one.

- Using the standard V_t equation will result in very low initial values. To correct this bias, correct each V_t using V_t = V_t / (1 - β^t). This has to be done per item so that the term (1 - β^t) is higher in the initial phase and approaches zero the more examples are processed. This is NOT equivalent to V_t = (β*V_t-1 + (1-β)*θt) / (1 - β^t). Bias correction is done every batch or epoch.

RMS prop
- Root mean square propagation uses the exponentially weighted derivatives to update the parameters and biases. To prevent dividing by zero and causing a run-time error, add an extremely small positive value. Here it is denoted as epsilon (ε).
S_dW = β*S_dW + (1 - β)*dW^2
W = W - α*(dW / (sqrt(S_dw) + ε))

- RMS propagation will minimize the effect of larger biases or parameters. The intent is to damp out oscillations of the gradient descent or loss function reduction. RMS prop and EWA reduce the effect of large values. One reason they work is that the initial values of parameters and biases are very far from the correct values. Minimizing their effect in the early stages of estimation reduces random and erroneous changes.

- RMS prop and similar weighted averaging algorithms allow a higher learning rate.

Adam Optimization
- Adam Optimization combines RMS prop with exponentially weighted averages to speed up gradient descent. It updates based on the exponentially averaged derivative with respect to w (V_dw) divided by the square root of the squared sum according to RMS prop (S_dW).
For each mini-batch:
V_dw = β_1 * V_dw + (1 - β_1)*dw
S_dw = β_2 * S_dw + (1 - β_2)*dw^2

Correct the value biases:
V_dw = V_dw/(1 - β_1)
S_dw = S_dw/(1 - β_2)

Update the parameter/matrix W:
W = W - α * (V_dw / (sqrt(S_dw) + ε))
*Use the same equations for db (derivative of the cost function with respect to the bias b)
*For RMS prop, moving averages, and Adam, the recommended β_1 is 0.9. For the squared sums, β_2 is recommended to be 0.99. The ε, according to the authors of Adam, is recommended to be 10^-8. These parameters are rarely tuned, unlike the learning rate.

Learning Rate Decay
- Learning rate decay reduces the learning rate over time. The assumption is that when the estimation is close to the minimum, a smaller learning rate is needed to avoid deviating too much due to data noise or chance.
α = (α / (1 + decayRate * epochNumber))*α_0

- Other formulas for the decay rate may be used. For example, the decay rate can be reduced by some discrete value after a certain number of epochs. Others use a constant k divided by the square root of the epoch count. The result is multiplied to α.

Local Optima
- For high-dimensional data, local optima are very rare. It is more common to have a set of parameters where the derivative is zero and changes to certain sets of parameters may guide the estimation to an acceptable or optimal minimum. It is rare to have a common convergence point where the derivative is zero and it would take excessive shifts that increase error to the levels of random initialization just to find the optimum gradient. A larger number of significant data inputs reduces the likelihood of a local minimum where the error of estimation cannot be reduced by ordinary parameter updates.

- Plateaus are still a problem with high-dimensional data since the derivative may stay at near-zero or zero for a long time despite the parameters being far from optimal. They can make gradient descent slow or give the illusion that it is safe to stop learning.

@@@
Hyper-parameter Tuning
- Sample values at random or using a coarse-to-fine process. Do not use a table or grid. It is hard to make a comprehensive grid of random values without sampling an impractical number of combinations. Sampling coarse-to-fine means sampling at random then gradually limiting your search to the combinations with the best performance.

- Make sure to sample using a scale that matches the possible hyper-parameter values. For values that may range from near-zero to one, use an exponential or logarithmic scale. Find your minimum and maximum values. Take the base ten logarithm of the start and end points then make a uniformly distributed random sample. The number of samples to make is arbitrary. E.g. for a value that may range from 0.0001 to 1, take the base-10 logarithm of equidistant values. Here, log_10 0.0001 = -4 and log_10 1 = 0. If you need to take five samples, use exponents ranging from -4, -3.2, -2.4...0.2, 1.

- Developers are free to determine the minimum and maximum values of some hyper-parameters such as the number of layers and the beta values of Adam.

@@@
Batch Normalization
- Batch normalization increases robustness versus hyper-parameter changes. Batch norm normalizes the input layer and even the out z or activation a of each layer. After normalizing the layer z or a values, change Z or A using gamma γ and beta β: z = γ*Z_norm + β. Gamma and beta are learnable parameters. Z is usually normalized using Z-score normalization. The normalized Z is often referred to as Z̃.
Z = Z - μ / sqrt(σ^2 + ε)
or
Z = (Z - μ) / σ

- When using batch normalization, you may omit the bias term. This term will have no effect on the result of z. The bias b is a constant added to all W * X products and batch norm uses Z-scores to normalize.

- Setting gamma (γ) and beta (β) to cancel out the mean and standard deviation will practically turn off batch norm. β may also be used as a replacement for the bias. These hyper-parameters have to be updated based on their derivatives similar to parameters W.

- Batch norm reduces random or noisy changes to the estimates by normalizing Z. It also makes weights deeper inside the neural network less affected by the weights in the much earlier layers. If the early layers have large shifts or incorrect weights, the effect of those weights is minimized. Especially during epochs close to initialization, changes have less effect on the weights of deeper layers. Since deeper layers are adjusting with some respect to the standard deviation and mean of previous layers activation values, mere changes in value do not always force a large change to the parameters of deeper layers. 

- One of the side-effects of batch-norm is a form of regularization when done with mini-batches. The mean and variance changes based on the batch used. Since the estimates of the mean and standard deviation are noisy themselves, downstream layers are slightly less likely to fit too much based on certain parameters or neuron activation values. This effect is similar to drop-out regularization, but it is NOT a good replacement for methods explicitly used for regularization.

- During testing, do not compute a new mean and variance for each batch. For the test set, use an exponential moving average that processes and accumulates the mean for all batches. The exponential moving average should be used to calculate z̃. This is because the testing phase usually requires each example to be fed into the model one at a time. As usual, this process will apply to all layers.

@@@
Multi-class Classification

Soft-max
a[L] = (e^Z[L]) / Σ(i = 1, n_layers)t_i
- Here, t_i is a vector containing the probabilities of each class, i.e. t_i = [e^Z|C_1, e^Z|C_2, e^Z|C_3...] where C_n denotes class n.
The output a[L] is a vector since Z is also a vector. Each activation value from the second-last layer will be treated as a value for which a parameter array W must be assigned. The parameters should be set so that e^Z / Σ(i = 1, n_layers)t_i is equivalent to the probabilities that the input data X[m] is a member of each one of the possible classes. 

Soft-max Loss
L(ŷ, y) = -Σ(j = 1, C)(y log_e(ŷ_j))
- Here, the loss function zeroes out all values except the correct value. If the prediction y-hat(ŷ) for the correct value y (1) is equal or close to one, the loss is low. log_e(1) = 0, 1 * 0 = -+0, loss is zero. Next, log_e(0.98) = -0.02020207... The lower ŷ is when y = 1, the larger the loss. log_e(0.1) = -2.3, loss is 2.3. This method is based on the maximum likelihood estimation process in statistics.

- Thus, the cost function for soft-max is:
J(W, b) = (1/m_examples) Σ(i = 1, m)L(ŷ, y)
J(W, b) = (1/m_examples) Σ(i = 1, m)(-Σ(j = 1, C)(y log_e(ŷ_j)))

Back-prop:
- Here, delta is used to denote a partial derivative.
δJ/δz = ŷ - y
or
J'(z(W, b)) = ŷ - y 

