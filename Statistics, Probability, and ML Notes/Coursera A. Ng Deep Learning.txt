++++++++++++++++++
General Guidelines

- When testing a formula or procedure, try the process using sample input. For equations, try to find a trend in the results relative to the input. For ML algorithms, run limited tests on groups or sets with at least thirty samples. Depending on the situation, a control may be needed or the same data has to be used for different processes.

- Numpy arrays and Pandas data frames list row count and "column" count in their shape like so: (n_rows, n_cols). Technically these are just nested arrays with two dimensions. Flattening them would show n_rows as elements. Each element holds n_cols sub-elements.

- To test if values meet conditions in Python, use the assert() method. E.g. assert(Z.shape[0] == w.shape) or assert Z.shape[1] == W.T.shape[1], "Z column count mismatch with W.transpose column count."

- Test validation and logic-checking are mandatory. For complicated functions or classes, more granular or step-by-step checks may be necessary.

- To allow codes scaffolding, you need to IMPORT the classes and methods.

@@@

Minimum Bayes Risk
- Minimum Bayes Risk is another accuracy metric. 
arg_max 1/n * Σ(Metric(E, E`))
here:
Metric may be a loss function or measure of similarity. Multiple translations have to be generated. For each candidate, get the score or metric relative to every other translation. This is a one vs. all comparison. Do the comparison for all translations and pick the one with the highest average similarity score. This assumes that all the translations taken together tend toward the correct translation.

Attention Model
- An attention model encourages a model to give greater weight to tokens that are at and near the current input. An RNN or BRNN will be used to compute features or encodings of the input sequence. After the entire sequence is processed, the resulting sequence of encodings will be processed by the attention model. The elements are processed sequentially. An attention parameter α (alpha) will be used to calculate the context for all tokens in the sequence. For each input element, a "context" is calculated using α and other tokens/encodings in the sequence. The context will used to generate an activation to predict a translation. When the next input element, i.e. encoding, is processed, the previous output is used along with a new context to generate a succeeding prediction. The attention parameter α has to be the same for the entire sequence.

Connectionist Temporal Classification (CTC) Cost Function for Speech Recognition
- The CTC is used for audio processing problems. The output of audio may be very long since the audio is sampled at a very high rate, e.g. several times a second. The output may have blanks where there is no meaningful audio. It will also have long repeated sequences of letters for stretches of audio lasting a few milliseconds that make up one letter. CTC collapses these repeated letters and blanks into human-readable transcripts. It may also be used in trigger-word detection. In trigger-word detection, the entire audio sequence that holds the trigger word is labelled as one or positive to counter the large number of non-trigger audio sequences.

@@@
Transformers

Self-attention
- Calculate the attention matrix of the current token:
A(q, K, V) = Σ_i ((exp(q*k<i>) / Σ_j exp(q*k<j>))*v<i>)
Here:
q (query), K (key), and V (value) are matrices computed using learnable weights. Each word or token in the sequence will be assigned a different set of W * q, W * k, and W * v matrices. The attention matrix A at the current token/timestamp A<t> will be calculated relative to its own set of parameters and vectors - W * q, W * k, and W * v - and the matrices K and V, which are the computed W*k and W*v vectors of other tokens in the sequence. Here, q, k, and v are representations/tokenized encodings of each word. When embedding, it is the convention to have q = k = v, i.e. the tokenized representation of a sequence is the same for the W_q, W_k, and W_v matrices. These encodings multiplied to W_q, Q_k, and W_v will result in Q, K, and V matrices for each word.

- A<t> will be computed as the soft-max of A(q, K, V)<t> vs all other tokens in the sequence, Σ_j A(q, K, V)<0,..j>. A of the current token may also be represented as A(q<t>, K, V). Take note that matrix multiplications are NOT commutative. The result of matrices A * B will be different from B * A. The resulting A<t> here will be a matrix of ratios. The softmax and A(q, K, V) computations will guarantee that information from all words in the sequence will influence A<t>, the matrix representation of the current token.

- A_1,...An will be computed in parallel for all tokens in the input of a transformer network. One advantage of this process is that A is not a fixed word embedding. The representation/encoding A of a token will not be based on a large corpus. Based on the sequence currently being processed, the model will generate a representation for the token.

- There is an alternative notation in literature for calculating the self-attention equation/matrix A of the current token. In the equation below, the denominator term is used to scale the dot product to prevent an explosion.
A(Q, K, V) = softmax(Q*K.transpose() / sqrt(d_k))V

Multi-head Attention
- In Multihead(Q, K, V), several instances of A(Q, K, V) will be run simultaneously for the current token vs. all other tokens in the sequence. The number of heads is a hyper-parameter. The concatenation of the results from the heads will be used to compute the output of the multi-head attention equation. The parameter matrices W are different for each head. Each head has different parameters W for each Q, K, and V. The goal is to have different versions of the Q, K, and V matrices per head.

Multihead(Q, K, V) = concat(A<head_1,...head_n>) * W_o
where:
A<head_n> = Attention(W_i*Q, W_i*K, W_i*V)

- The full transformer network has separate encoding and decoding stages. The input sequence is turned into a basic vector encoding and then passed to the encoder stage. The encoding should include the position of the token in the sequence. In the encoding stage, the Q, K, and V values of all the tokens will be computed. First, a multi-head attention calculation will be performed for each of the input tokens. The results will be passed to a feed-forward neural network. Encoder layers of this structure will be stacked or repeated N times. The paper recommends N=6 stacked encoder layers. After the encoding stage runs N times, the results will be passed to the decoder. In the decoder, the start-of-sentence token will be fed in at the beginning. The Q matrix will be calculated by the first multi-head block of the decoder based on the start-of-sentence (<SOS>) token. In the next block, the Q matrix of the first decoder block will be processed with the K and V matrices from the encoder in another multi-head attention layer. The decoder will feed the result to a feed-forward neural network, usually with a ReLU activation. Decoder layers will also be stacked/repeated N times. Finally, the decoder will output the next token after <SOS> using a linear layer with a softmax activation to predict a translation. This output translation will be passed back into the beginning of the decoder block so that the next token can be predicted. The decoding cycle will repeat until the end-of-sentence <EOS> token is reached.

- In the decoding stage, the true or correct next token is hidden from the decoder to prevent information leakage. While a certain token is being processed, the correct tokens after it are hidden. The correct previous tokens may also be provided during training. Hiding the correct future tokens is referred to in the paper as "masked multi-headed attention."

- The position of a word is also added to enrich context. The vector encoding the token position will be added directly to the input vector before starting the encoding loop.
PE(pos, 2i) = sin(pos / 10000 * (2i/d))
and
PE(pos, 2i + 1) = cos(pos / 10000 * (2i/d))
in both:
pos = Numeric position of the current token in the sequence.
i = Current position being calculated for the PE vector, 0 - PE.length, depends on network shape.
These will be represented as vectors.

- After every multi-head and neural network block, residuals will be added and normalization will be run in an "Add[ition] & Norm[alization]" layer. This has an effect similar to batch normalizaion. It makes gradient descent smoother.

@@@
Transformer Network Encoder and Decoder Architecture According to "Attention is All You Need"
- Here, the encoder maps an input sequence of symbol representations (x1 , ..., xn ) to a sequence
of continuous representations z = (z1 , ..., zn ). Given z, the decoder then generates an output
sequence (y1 , ..., ym ) of symbols one element at a time. At each step the model is auto-regressive
[9], consuming the previously generated symbols as additional input when generating the next.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder

Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [10] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512.
*Note: Here, 512 is the dimension of the demonstration model. It is not a standard.

Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position i can depend only on the known outputs at positions less than i.
@@@

Teacher Forcing
- In "teacher forcing", the true label of a previous token is used as the input to the decoder regardless of whether it got the last token right. This is to avoid a cascade of errors. This forcing may be gradually reduced as training continues.

Transformers vs. RNNs and BRNNs
- RNNs, despite the implementation of attention, may still lose information the further tokens are from each other. Sequential models process tokens one at a time, so there are less ways to parallelize their implementation. With RNNs and especially deep RNNs, there are more steps taken during back-propagation than transformers. RNNs perform back-propagation for each input token. Transformers take around one step from output to gradients during a cycle of back-propagation. Last, transformers are less likely to cause vanishing gradients even vs. LSTM and GRU transformer nets.

